{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ：面向预训练 Transformer 模型设计的量化技术（ICLR 2023）\n",
    "GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers 是一个高效、精准的\n",
    "量化技术，特别适用于大规模GPT模型，能够在显著降低模型大小和计算需求的同时，保持高准确度和推理速度。\n",
    "***\n",
    "GPTQ算法具有以下技术特点：\\\n",
    "\\\n",
    "1.专为GPT模型设计：GPTQ针对大规模GPT模型（如1750亿参数规模的模型）进行优化，解决了这类模型因\n",
    "规模庞大导致的高计算和存储成本问题。\\\n",
    "2.一次性权重量化方法：GPTQ是一种基于近似二阶信息的权重量化方法，能够在一次处理中完成模型的量化。\\\n",
    "3.高效率：GPTQ能在大约四个GPU小时内完成1750亿参数的GPT模型的量化。\\\n",
    "4.低位宽量化：通过将权重位宽降至每个权重3或4位，GPTQ显著减少了模型的大小。\\\n",
    "5.准确度保持：即便在进行显著的位宽减少后，GPTQ也能保持与未压缩模型相近的准确度，减少性能损失。\\\n",
    "6.支持极端量化：GPTQ还可以实现更极端的量化，如2位或三元量化，同时保持合理的准确度。\\\n",
    "7.推理速度提升：使用GPTQ量化的模型在高端GPU（如NVIDIA A100）上实现了大约3.25倍的推理速度提升，\n",
    "在成本效益更高的GPU（如NVIDIA A6000）上实现了大约4.5倍的速度提升。\\\n",
    "8.适用于单GPU环境：GPTQ使得在单个GPU内执行大规模模型的生成推理成为可能，显著降低了部署这类模\n",
    "型的硬件要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304084405.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ 量化算法核心流程\n",
    "核心步骤：使用存储在Cholesky（切尔斯基）分解中的逆Hessian（海森）\n",
    "信息量化连续列的块（加粗表示），并在步骤结束时更新剩余的权重\n",
    "（蓝色表示），在每个块内递归（白色中间块）地应用量化过程。\\\n",
    "GPTQ量化过程的关键步骤操作，具体描述如下：\\\n",
    "1.块量化：选择一块连续的列（在图中加粗表示），并将其作为当前步骤\n",
    "的量化目标。\\\n",
    "2.使用Cholesky分解：利用Cholesky分解得到的逆Hessian信息来量化选定的块。Cholesky分解提供了一种数值稳定的方法来处理逆矩阵，这对于维\n",
    "持量化过程的准确性至关重要。\\\n",
    "3.权重更新：在每个量化步骤的最后，更新剩余的权重（在图中以蓝色表\n",
    "示）。这个步骤确保了整个量化过程的连贯性和精确性。\\\n",
    "4.递归量化：在每个选定的块内部，量化过程是递归应用的。这意味着量\n",
    "化过程首先聚焦于一个较小的子块，然后逐步扩展到整个块。\n",
    "通过这种方式，GPTQ方法能够在保持高度精度的同时，高效地处理大量\n",
    "的权重，这对于大型模型的量化至关重要。这种策略特别适用于处理大\n",
    "型、复杂的模型，如GPT系列，其中权重数量巨大，且量化过程需要特别\n",
    "小心以避免精度损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活感知权重量化（Activation-aware Weight Quantization, AWQ）\n",
    "激活感知权重量化（AWQ）算法，其原理不是对模型中的所有权重进行量化，而是仅保留小部分（1%）对LLM性能\\\n",
    "至关重要的权重。其算法主要特点如下：\\\n",
    "1.低位权重量化：AWQ专为大型语言模型（LLMs）设计，支持低位（即少位数）的权重量化，有效减少模型大小。\\\n",
    "2.重点保护显著权重：AWQ基于权重重要性不均的观察，只需保护大约1%的显著权重，即可显著减少量化误差。\\\n",
    "3.观察激活而非权重：在确定哪些权重是显著的过程中，AWQ通过观察激活分布而非权重分布来进行。\\\n",
    "4.无需反向传播或重构：AWQ不依赖于复杂的反向传播或重构过程，因此能够更好地保持模型的泛化能力，避免对\n",
    "特定数据集的过拟合。\\\n",
    "5.适用于多种模型和任务：AWQ在多种语言建模任务和领域特定基准测试中表现出色，包括指令调整的语言模型和\n",
    "多模态语言模型。\\\n",
    "6.高效的推理框架：与AWQ配套的是一个为LLMs量身定做的高效推理框架，提供显著的速度提升，适用于桌面和\n",
    "移动GPU。\\\n",
    "7.支持边缘设备部署：这种方法支持在内存和计算能力有限的边缘设备（如NVIDIA Jetson Orin 64GB）上部署大\n",
    "型模型，如70B Llama-2模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304092755.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304093236.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitsAndBytes 简介\n",
    "BitsAndBytes（BNB）是自定义CUDA函数的轻量级包装器，特别是8比特优化器、矩阵乘法和量\n",
    "化函数。主要特征如下：\\\n",
    " \\\n",
    "•具有混合精度分解的8比特矩阵乘法\\\n",
    "•LLM.int8()推理\\\n",
    "•8比特优化器：Adam、AdamW、RMSProp、LARS、LAMB、Lion（节省75%内存）\\\n",
    "•稳定的嵌入层：通过更好的初始化和标准化提高稳定性\\\n",
    "•8比特量化：分位数、线性和动态量化\\\n",
    "•快速的分位数估计：比其他算法快100倍\\\n",
    " \\\n",
    "在 Transformers 量化方案中，BNB 是将模型量化为8位和4位的最简单选择。\\\n",
    " \\\n",
    "•8位量化将fp16中的异常值与int8中的非异常值相乘，将非异常值转换回fp16，然后将它们相加以\n",
    "返回fp16中的权重。这减少了异常值对模型性能产生的降级效果。\\\n",
    "•4位量化进一步压缩了模型，并且通常与QLoRA一起用于微调量化LLM（低精度语言模型）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained('E:\\model\\language\\opt-125m',trust_remote_code=True,device_map='auto').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结 AWQ yyds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码环节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,AwqConfig\n",
    "import torch\n",
    "model_path='/root/share/model_repos/internlm-7b'\n",
    "quant_path='/root/phb/quant/internlm-7b'\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config=AwqConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    zero_point=True,\n",
    "    version='gemm'\n",
    ").to_dict()\n",
    "model.model.config.quantization_config=quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/phb/quant/internlm-7b/tokenizer_config.json',\n",
       " '/root/phb/quant/internlm-7b/special_tokens_map.json',\n",
       " '/root/phb/quant/internlm-7b/./tokenizer.model',\n",
       " '/root/phb/quant/internlm-7b/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of InternLMForCausalLM(\n",
       "  (model): InternLMModel(\n",
       "    (embed_tokens): Embedding(103168, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x InternLMDecoderLayer(\n",
       "        (self_attn): InternLMAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): InternLMRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): InternLMMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): InternLMRMSNorm()\n",
       "        (post_attention_layernorm): InternLMRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLMRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=103168, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,GPTQConfig\n",
    "import torch\n",
    "model_path='/root/share/model_repos/internlm-7b'\n",
    "quant_path='/root/phb/quant/internlm-7b'\n",
    "quant_config=GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    dataset=[\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"],\n",
    "    desc_act=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n",
      "Quantizing model.layers blocks : 100%|██████████| 32/32 [06:55<00:00, 12.97s/it]\n",
      "Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n",
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:556\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/modeling_utils.py:3561\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m     dispatch_model(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3561\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3562\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[1;32m   3564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/quantizers/base.py:179\u001b[0m, in \u001b[0;36mHfQuantizer.postprocess_model\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Post-process the model post weights loading.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    Make sure to override the abstract method `_process_model_after_weight_loading`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m            The keyword arguments that are passed along `_process_model_after_weight_loading`.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_model_after_weight_loading\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/transformers/quantizers/quantizer_gptq.py:85\u001b[0m, in \u001b[0;36mGptqHfQuantizer._process_model_after_weight_loading\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mname_or_path\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimum_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m GPTQConfig\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimum_quantizer\u001b[38;5;241m.\u001b[39mto_dict())\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/optimum/gptq/quantizer.py:561\u001b[0m, in \u001b[0;36mGPTQQuantizer.quantize_model\u001b[0;34m(self, model, tokenizer)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_exllama \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Step 4: Pack the model at the end (Replacing the layers)\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m model\u001b[38;5;241m.\u001b[39mis_quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    564\u001b[0m model\u001b[38;5;241m.\u001b[39mquantization_method \u001b[38;5;241m=\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/optimum/gptq/quantizer.py:640\u001b[0m, in \u001b[0;36mGPTQQuantizer.pack_model\u001b[0;34m(self, model, quantizers)\u001b[0m\n\u001b[1;32m    638\u001b[0m     qlayers[name]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    639\u001b[0m     layers[name], scale, zero, g_idx \u001b[38;5;241m=\u001b[39m layers[name]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), scale\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), zero\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), g_idx\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 640\u001b[0m     \u001b[43mqlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     qlayers[name]\u001b[38;5;241m.\u001b[39mto(layer_device)\n\u001b[1;32m    643\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel packed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/auto_gptq/nn_modules/qlinear/qlinear_cuda_old.py:127\u001b[0m, in \u001b[0;36mQuantLinear.pack\u001b[0;34m(self, linear, scales, zeros, g_idx)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfeatures):\n\u001b[1;32m    126\u001b[0m     g_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_size\n\u001b[0;32m--> 127\u001b[0m     intweight\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mround((W[:, idx] \u001b[38;5;241m+\u001b[39m scale_zeros[g_idx]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscales\u001b[49m[g_idx])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint)[:, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m    128\u001b[0m intweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(intweight, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    129\u001b[0m intweight \u001b[38;5;241m=\u001b[39m intweight\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/.conda/envs/internlm-demo/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_path,quantization_config=quant_config,trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,AwqConfig\n",
    "import torch\n",
    "model_path='/root/share/model_repos/internlm-7b'\n",
    "quant_path='/root/phb/quant/internlm-7b'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.84it/s]\n",
      "Some weights of the model checkpoint at /root/phb/quant/internlm-7b were not used when initializing InternLMForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing InternLMForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing InternLMForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of InternLMForCausalLM were not initialized from the model checkpoint at /root/phb/quant/internlm-7b and are newly initialized: ['model.layers.0.mlp.down_proj.qweight', 'model.layers.0.mlp.down_proj.qzeros', 'model.layers.0.mlp.down_proj.scales', 'model.layers.0.mlp.gate_proj.qweight', 'model.layers.0.mlp.gate_proj.qzeros', 'model.layers.0.mlp.gate_proj.scales', 'model.layers.0.mlp.up_proj.qweight', 'model.layers.0.mlp.up_proj.qzeros', 'model.layers.0.mlp.up_proj.scales', 'model.layers.0.self_attn.k_proj.qweight', 'model.layers.0.self_attn.k_proj.qzeros', 'model.layers.0.self_attn.k_proj.scales', 'model.layers.0.self_attn.o_proj.qweight', 'model.layers.0.self_attn.o_proj.qzeros', 'model.layers.0.self_attn.o_proj.scales', 'model.layers.0.self_attn.q_proj.qweight', 'model.layers.0.self_attn.q_proj.qzeros', 'model.layers.0.self_attn.q_proj.scales', 'model.layers.0.self_attn.v_proj.qweight', 'model.layers.0.self_attn.v_proj.qzeros', 'model.layers.0.self_attn.v_proj.scales', 'model.layers.1.mlp.down_proj.qweight', 'model.layers.1.mlp.down_proj.qzeros', 'model.layers.1.mlp.down_proj.scales', 'model.layers.1.mlp.gate_proj.qweight', 'model.layers.1.mlp.gate_proj.qzeros', 'model.layers.1.mlp.gate_proj.scales', 'model.layers.1.mlp.up_proj.qweight', 'model.layers.1.mlp.up_proj.qzeros', 'model.layers.1.mlp.up_proj.scales', 'model.layers.1.self_attn.k_proj.qweight', 'model.layers.1.self_attn.k_proj.qzeros', 'model.layers.1.self_attn.k_proj.scales', 'model.layers.1.self_attn.o_proj.qweight', 'model.layers.1.self_attn.o_proj.qzeros', 'model.layers.1.self_attn.o_proj.scales', 'model.layers.1.self_attn.q_proj.qweight', 'model.layers.1.self_attn.q_proj.qzeros', 'model.layers.1.self_attn.q_proj.scales', 'model.layers.1.self_attn.v_proj.qweight', 'model.layers.1.self_attn.v_proj.qzeros', 'model.layers.1.self_attn.v_proj.scales', 'model.layers.10.mlp.down_proj.qweight', 'model.layers.10.mlp.down_proj.qzeros', 'model.layers.10.mlp.down_proj.scales', 'model.layers.10.mlp.gate_proj.qweight', 'model.layers.10.mlp.gate_proj.qzeros', 'model.layers.10.mlp.gate_proj.scales', 'model.layers.10.mlp.up_proj.qweight', 'model.layers.10.mlp.up_proj.qzeros', 'model.layers.10.mlp.up_proj.scales', 'model.layers.10.self_attn.k_proj.qweight', 'model.layers.10.self_attn.k_proj.qzeros', 'model.layers.10.self_attn.k_proj.scales', 'model.layers.10.self_attn.o_proj.qweight', 'model.layers.10.self_attn.o_proj.qzeros', 'model.layers.10.self_attn.o_proj.scales', 'model.layers.10.self_attn.q_proj.qweight', 'model.layers.10.self_attn.q_proj.qzeros', 'model.layers.10.self_attn.q_proj.scales', 'model.layers.10.self_attn.v_proj.qweight', 'model.layers.10.self_attn.v_proj.qzeros', 'model.layers.10.self_attn.v_proj.scales', 'model.layers.11.mlp.down_proj.qweight', 'model.layers.11.mlp.down_proj.qzeros', 'model.layers.11.mlp.down_proj.scales', 'model.layers.11.mlp.gate_proj.qweight', 'model.layers.11.mlp.gate_proj.qzeros', 'model.layers.11.mlp.gate_proj.scales', 'model.layers.11.mlp.up_proj.qweight', 'model.layers.11.mlp.up_proj.qzeros', 'model.layers.11.mlp.up_proj.scales', 'model.layers.11.self_attn.k_proj.qweight', 'model.layers.11.self_attn.k_proj.qzeros', 'model.layers.11.self_attn.k_proj.scales', 'model.layers.11.self_attn.o_proj.qweight', 'model.layers.11.self_attn.o_proj.qzeros', 'model.layers.11.self_attn.o_proj.scales', 'model.layers.11.self_attn.q_proj.qweight', 'model.layers.11.self_attn.q_proj.qzeros', 'model.layers.11.self_attn.q_proj.scales', 'model.layers.11.self_attn.v_proj.qweight', 'model.layers.11.self_attn.v_proj.qzeros', 'model.layers.11.self_attn.v_proj.scales', 'model.layers.12.mlp.down_proj.qweight', 'model.layers.12.mlp.down_proj.qzeros', 'model.layers.12.mlp.down_proj.scales', 'model.layers.12.mlp.gate_proj.qweight', 'model.layers.12.mlp.gate_proj.qzeros', 'model.layers.12.mlp.gate_proj.scales', 'model.layers.12.mlp.up_proj.qweight', 'model.layers.12.mlp.up_proj.qzeros', 'model.layers.12.mlp.up_proj.scales', 'model.layers.12.self_attn.k_proj.qweight', 'model.layers.12.self_attn.k_proj.qzeros', 'model.layers.12.self_attn.k_proj.scales', 'model.layers.12.self_attn.o_proj.qweight', 'model.layers.12.self_attn.o_proj.qzeros', 'model.layers.12.self_attn.o_proj.scales', 'model.layers.12.self_attn.q_proj.qweight', 'model.layers.12.self_attn.q_proj.qzeros', 'model.layers.12.self_attn.q_proj.scales', 'model.layers.12.self_attn.v_proj.qweight', 'model.layers.12.self_attn.v_proj.qzeros', 'model.layers.12.self_attn.v_proj.scales', 'model.layers.13.mlp.down_proj.qweight', 'model.layers.13.mlp.down_proj.qzeros', 'model.layers.13.mlp.down_proj.scales', 'model.layers.13.mlp.gate_proj.qweight', 'model.layers.13.mlp.gate_proj.qzeros', 'model.layers.13.mlp.gate_proj.scales', 'model.layers.13.mlp.up_proj.qweight', 'model.layers.13.mlp.up_proj.qzeros', 'model.layers.13.mlp.up_proj.scales', 'model.layers.13.self_attn.k_proj.qweight', 'model.layers.13.self_attn.k_proj.qzeros', 'model.layers.13.self_attn.k_proj.scales', 'model.layers.13.self_attn.o_proj.qweight', 'model.layers.13.self_attn.o_proj.qzeros', 'model.layers.13.self_attn.o_proj.scales', 'model.layers.13.self_attn.q_proj.qweight', 'model.layers.13.self_attn.q_proj.qzeros', 'model.layers.13.self_attn.q_proj.scales', 'model.layers.13.self_attn.v_proj.qweight', 'model.layers.13.self_attn.v_proj.qzeros', 'model.layers.13.self_attn.v_proj.scales', 'model.layers.14.mlp.down_proj.qweight', 'model.layers.14.mlp.down_proj.qzeros', 'model.layers.14.mlp.down_proj.scales', 'model.layers.14.mlp.gate_proj.qweight', 'model.layers.14.mlp.gate_proj.qzeros', 'model.layers.14.mlp.gate_proj.scales', 'model.layers.14.mlp.up_proj.qweight', 'model.layers.14.mlp.up_proj.qzeros', 'model.layers.14.mlp.up_proj.scales', 'model.layers.14.self_attn.k_proj.qweight', 'model.layers.14.self_attn.k_proj.qzeros', 'model.layers.14.self_attn.k_proj.scales', 'model.layers.14.self_attn.o_proj.qweight', 'model.layers.14.self_attn.o_proj.qzeros', 'model.layers.14.self_attn.o_proj.scales', 'model.layers.14.self_attn.q_proj.qweight', 'model.layers.14.self_attn.q_proj.qzeros', 'model.layers.14.self_attn.q_proj.scales', 'model.layers.14.self_attn.v_proj.qweight', 'model.layers.14.self_attn.v_proj.qzeros', 'model.layers.14.self_attn.v_proj.scales', 'model.layers.15.mlp.down_proj.qweight', 'model.layers.15.mlp.down_proj.qzeros', 'model.layers.15.mlp.down_proj.scales', 'model.layers.15.mlp.gate_proj.qweight', 'model.layers.15.mlp.gate_proj.qzeros', 'model.layers.15.mlp.gate_proj.scales', 'model.layers.15.mlp.up_proj.qweight', 'model.layers.15.mlp.up_proj.qzeros', 'model.layers.15.mlp.up_proj.scales', 'model.layers.15.self_attn.k_proj.qweight', 'model.layers.15.self_attn.k_proj.qzeros', 'model.layers.15.self_attn.k_proj.scales', 'model.layers.15.self_attn.o_proj.qweight', 'model.layers.15.self_attn.o_proj.qzeros', 'model.layers.15.self_attn.o_proj.scales', 'model.layers.15.self_attn.q_proj.qweight', 'model.layers.15.self_attn.q_proj.qzeros', 'model.layers.15.self_attn.q_proj.scales', 'model.layers.15.self_attn.v_proj.qweight', 'model.layers.15.self_attn.v_proj.qzeros', 'model.layers.15.self_attn.v_proj.scales', 'model.layers.16.mlp.down_proj.qweight', 'model.layers.16.mlp.down_proj.qzeros', 'model.layers.16.mlp.down_proj.scales', 'model.layers.16.mlp.gate_proj.qweight', 'model.layers.16.mlp.gate_proj.qzeros', 'model.layers.16.mlp.gate_proj.scales', 'model.layers.16.mlp.up_proj.qweight', 'model.layers.16.mlp.up_proj.qzeros', 'model.layers.16.mlp.up_proj.scales', 'model.layers.16.self_attn.k_proj.qweight', 'model.layers.16.self_attn.k_proj.qzeros', 'model.layers.16.self_attn.k_proj.scales', 'model.layers.16.self_attn.o_proj.qweight', 'model.layers.16.self_attn.o_proj.qzeros', 'model.layers.16.self_attn.o_proj.scales', 'model.layers.16.self_attn.q_proj.qweight', 'model.layers.16.self_attn.q_proj.qzeros', 'model.layers.16.self_attn.q_proj.scales', 'model.layers.16.self_attn.v_proj.qweight', 'model.layers.16.self_attn.v_proj.qzeros', 'model.layers.16.self_attn.v_proj.scales', 'model.layers.17.mlp.down_proj.qweight', 'model.layers.17.mlp.down_proj.qzeros', 'model.layers.17.mlp.down_proj.scales', 'model.layers.17.mlp.gate_proj.qweight', 'model.layers.17.mlp.gate_proj.qzeros', 'model.layers.17.mlp.gate_proj.scales', 'model.layers.17.mlp.up_proj.qweight', 'model.layers.17.mlp.up_proj.qzeros', 'model.layers.17.mlp.up_proj.scales', 'model.layers.17.self_attn.k_proj.qweight', 'model.layers.17.self_attn.k_proj.qzeros', 'model.layers.17.self_attn.k_proj.scales', 'model.layers.17.self_attn.o_proj.qweight', 'model.layers.17.self_attn.o_proj.qzeros', 'model.layers.17.self_attn.o_proj.scales', 'model.layers.17.self_attn.q_proj.qweight', 'model.layers.17.self_attn.q_proj.qzeros', 'model.layers.17.self_attn.q_proj.scales', 'model.layers.17.self_attn.v_proj.qweight', 'model.layers.17.self_attn.v_proj.qzeros', 'model.layers.17.self_attn.v_proj.scales', 'model.layers.18.mlp.down_proj.qweight', 'model.layers.18.mlp.down_proj.qzeros', 'model.layers.18.mlp.down_proj.scales', 'model.layers.18.mlp.gate_proj.qweight', 'model.layers.18.mlp.gate_proj.qzeros', 'model.layers.18.mlp.gate_proj.scales', 'model.layers.18.mlp.up_proj.qweight', 'model.layers.18.mlp.up_proj.qzeros', 'model.layers.18.mlp.up_proj.scales', 'model.layers.18.self_attn.k_proj.qweight', 'model.layers.18.self_attn.k_proj.qzeros', 'model.layers.18.self_attn.k_proj.scales', 'model.layers.18.self_attn.o_proj.qweight', 'model.layers.18.self_attn.o_proj.qzeros', 'model.layers.18.self_attn.o_proj.scales', 'model.layers.18.self_attn.q_proj.qweight', 'model.layers.18.self_attn.q_proj.qzeros', 'model.layers.18.self_attn.q_proj.scales', 'model.layers.18.self_attn.v_proj.qweight', 'model.layers.18.self_attn.v_proj.qzeros', 'model.layers.18.self_attn.v_proj.scales', 'model.layers.19.mlp.down_proj.qweight', 'model.layers.19.mlp.down_proj.qzeros', 'model.layers.19.mlp.down_proj.scales', 'model.layers.19.mlp.gate_proj.qweight', 'model.layers.19.mlp.gate_proj.qzeros', 'model.layers.19.mlp.gate_proj.scales', 'model.layers.19.mlp.up_proj.qweight', 'model.layers.19.mlp.up_proj.qzeros', 'model.layers.19.mlp.up_proj.scales', 'model.layers.19.self_attn.k_proj.qweight', 'model.layers.19.self_attn.k_proj.qzeros', 'model.layers.19.self_attn.k_proj.scales', 'model.layers.19.self_attn.o_proj.qweight', 'model.layers.19.self_attn.o_proj.qzeros', 'model.layers.19.self_attn.o_proj.scales', 'model.layers.19.self_attn.q_proj.qweight', 'model.layers.19.self_attn.q_proj.qzeros', 'model.layers.19.self_attn.q_proj.scales', 'model.layers.19.self_attn.v_proj.qweight', 'model.layers.19.self_attn.v_proj.qzeros', 'model.layers.19.self_attn.v_proj.scales', 'model.layers.2.mlp.down_proj.qweight', 'model.layers.2.mlp.down_proj.qzeros', 'model.layers.2.mlp.down_proj.scales', 'model.layers.2.mlp.gate_proj.qweight', 'model.layers.2.mlp.gate_proj.qzeros', 'model.layers.2.mlp.gate_proj.scales', 'model.layers.2.mlp.up_proj.qweight', 'model.layers.2.mlp.up_proj.qzeros', 'model.layers.2.mlp.up_proj.scales', 'model.layers.2.self_attn.k_proj.qweight', 'model.layers.2.self_attn.k_proj.qzeros', 'model.layers.2.self_attn.k_proj.scales', 'model.layers.2.self_attn.o_proj.qweight', 'model.layers.2.self_attn.o_proj.qzeros', 'model.layers.2.self_attn.o_proj.scales', 'model.layers.2.self_attn.q_proj.qweight', 'model.layers.2.self_attn.q_proj.qzeros', 'model.layers.2.self_attn.q_proj.scales', 'model.layers.2.self_attn.v_proj.qweight', 'model.layers.2.self_attn.v_proj.qzeros', 'model.layers.2.self_attn.v_proj.scales', 'model.layers.20.mlp.down_proj.qweight', 'model.layers.20.mlp.down_proj.qzeros', 'model.layers.20.mlp.down_proj.scales', 'model.layers.20.mlp.gate_proj.qweight', 'model.layers.20.mlp.gate_proj.qzeros', 'model.layers.20.mlp.gate_proj.scales', 'model.layers.20.mlp.up_proj.qweight', 'model.layers.20.mlp.up_proj.qzeros', 'model.layers.20.mlp.up_proj.scales', 'model.layers.20.self_attn.k_proj.qweight', 'model.layers.20.self_attn.k_proj.qzeros', 'model.layers.20.self_attn.k_proj.scales', 'model.layers.20.self_attn.o_proj.qweight', 'model.layers.20.self_attn.o_proj.qzeros', 'model.layers.20.self_attn.o_proj.scales', 'model.layers.20.self_attn.q_proj.qweight', 'model.layers.20.self_attn.q_proj.qzeros', 'model.layers.20.self_attn.q_proj.scales', 'model.layers.20.self_attn.v_proj.qweight', 'model.layers.20.self_attn.v_proj.qzeros', 'model.layers.20.self_attn.v_proj.scales', 'model.layers.21.mlp.down_proj.qweight', 'model.layers.21.mlp.down_proj.qzeros', 'model.layers.21.mlp.down_proj.scales', 'model.layers.21.mlp.gate_proj.qweight', 'model.layers.21.mlp.gate_proj.qzeros', 'model.layers.21.mlp.gate_proj.scales', 'model.layers.21.mlp.up_proj.qweight', 'model.layers.21.mlp.up_proj.qzeros', 'model.layers.21.mlp.up_proj.scales', 'model.layers.21.self_attn.k_proj.qweight', 'model.layers.21.self_attn.k_proj.qzeros', 'model.layers.21.self_attn.k_proj.scales', 'model.layers.21.self_attn.o_proj.qweight', 'model.layers.21.self_attn.o_proj.qzeros', 'model.layers.21.self_attn.o_proj.scales', 'model.layers.21.self_attn.q_proj.qweight', 'model.layers.21.self_attn.q_proj.qzeros', 'model.layers.21.self_attn.q_proj.scales', 'model.layers.21.self_attn.v_proj.qweight', 'model.layers.21.self_attn.v_proj.qzeros', 'model.layers.21.self_attn.v_proj.scales', 'model.layers.22.mlp.down_proj.qweight', 'model.layers.22.mlp.down_proj.qzeros', 'model.layers.22.mlp.down_proj.scales', 'model.layers.22.mlp.gate_proj.qweight', 'model.layers.22.mlp.gate_proj.qzeros', 'model.layers.22.mlp.gate_proj.scales', 'model.layers.22.mlp.up_proj.qweight', 'model.layers.22.mlp.up_proj.qzeros', 'model.layers.22.mlp.up_proj.scales', 'model.layers.22.self_attn.k_proj.qweight', 'model.layers.22.self_attn.k_proj.qzeros', 'model.layers.22.self_attn.k_proj.scales', 'model.layers.22.self_attn.o_proj.qweight', 'model.layers.22.self_attn.o_proj.qzeros', 'model.layers.22.self_attn.o_proj.scales', 'model.layers.22.self_attn.q_proj.qweight', 'model.layers.22.self_attn.q_proj.qzeros', 'model.layers.22.self_attn.q_proj.scales', 'model.layers.22.self_attn.v_proj.qweight', 'model.layers.22.self_attn.v_proj.qzeros', 'model.layers.22.self_attn.v_proj.scales', 'model.layers.23.mlp.down_proj.qweight', 'model.layers.23.mlp.down_proj.qzeros', 'model.layers.23.mlp.down_proj.scales', 'model.layers.23.mlp.gate_proj.qweight', 'model.layers.23.mlp.gate_proj.qzeros', 'model.layers.23.mlp.gate_proj.scales', 'model.layers.23.mlp.up_proj.qweight', 'model.layers.23.mlp.up_proj.qzeros', 'model.layers.23.mlp.up_proj.scales', 'model.layers.23.self_attn.k_proj.qweight', 'model.layers.23.self_attn.k_proj.qzeros', 'model.layers.23.self_attn.k_proj.scales', 'model.layers.23.self_attn.o_proj.qweight', 'model.layers.23.self_attn.o_proj.qzeros', 'model.layers.23.self_attn.o_proj.scales', 'model.layers.23.self_attn.q_proj.qweight', 'model.layers.23.self_attn.q_proj.qzeros', 'model.layers.23.self_attn.q_proj.scales', 'model.layers.23.self_attn.v_proj.qweight', 'model.layers.23.self_attn.v_proj.qzeros', 'model.layers.23.self_attn.v_proj.scales', 'model.layers.24.mlp.down_proj.qweight', 'model.layers.24.mlp.down_proj.qzeros', 'model.layers.24.mlp.down_proj.scales', 'model.layers.24.mlp.gate_proj.qweight', 'model.layers.24.mlp.gate_proj.qzeros', 'model.layers.24.mlp.gate_proj.scales', 'model.layers.24.mlp.up_proj.qweight', 'model.layers.24.mlp.up_proj.qzeros', 'model.layers.24.mlp.up_proj.scales', 'model.layers.24.self_attn.k_proj.qweight', 'model.layers.24.self_attn.k_proj.qzeros', 'model.layers.24.self_attn.k_proj.scales', 'model.layers.24.self_attn.o_proj.qweight', 'model.layers.24.self_attn.o_proj.qzeros', 'model.layers.24.self_attn.o_proj.scales', 'model.layers.24.self_attn.q_proj.qweight', 'model.layers.24.self_attn.q_proj.qzeros', 'model.layers.24.self_attn.q_proj.scales', 'model.layers.24.self_attn.v_proj.qweight', 'model.layers.24.self_attn.v_proj.qzeros', 'model.layers.24.self_attn.v_proj.scales', 'model.layers.25.mlp.down_proj.qweight', 'model.layers.25.mlp.down_proj.qzeros', 'model.layers.25.mlp.down_proj.scales', 'model.layers.25.mlp.gate_proj.qweight', 'model.layers.25.mlp.gate_proj.qzeros', 'model.layers.25.mlp.gate_proj.scales', 'model.layers.25.mlp.up_proj.qweight', 'model.layers.25.mlp.up_proj.qzeros', 'model.layers.25.mlp.up_proj.scales', 'model.layers.25.self_attn.k_proj.qweight', 'model.layers.25.self_attn.k_proj.qzeros', 'model.layers.25.self_attn.k_proj.scales', 'model.layers.25.self_attn.o_proj.qweight', 'model.layers.25.self_attn.o_proj.qzeros', 'model.layers.25.self_attn.o_proj.scales', 'model.layers.25.self_attn.q_proj.qweight', 'model.layers.25.self_attn.q_proj.qzeros', 'model.layers.25.self_attn.q_proj.scales', 'model.layers.25.self_attn.v_proj.qweight', 'model.layers.25.self_attn.v_proj.qzeros', 'model.layers.25.self_attn.v_proj.scales', 'model.layers.26.mlp.down_proj.qweight', 'model.layers.26.mlp.down_proj.qzeros', 'model.layers.26.mlp.down_proj.scales', 'model.layers.26.mlp.gate_proj.qweight', 'model.layers.26.mlp.gate_proj.qzeros', 'model.layers.26.mlp.gate_proj.scales', 'model.layers.26.mlp.up_proj.qweight', 'model.layers.26.mlp.up_proj.qzeros', 'model.layers.26.mlp.up_proj.scales', 'model.layers.26.self_attn.k_proj.qweight', 'model.layers.26.self_attn.k_proj.qzeros', 'model.layers.26.self_attn.k_proj.scales', 'model.layers.26.self_attn.o_proj.qweight', 'model.layers.26.self_attn.o_proj.qzeros', 'model.layers.26.self_attn.o_proj.scales', 'model.layers.26.self_attn.q_proj.qweight', 'model.layers.26.self_attn.q_proj.qzeros', 'model.layers.26.self_attn.q_proj.scales', 'model.layers.26.self_attn.v_proj.qweight', 'model.layers.26.self_attn.v_proj.qzeros', 'model.layers.26.self_attn.v_proj.scales', 'model.layers.27.mlp.down_proj.qweight', 'model.layers.27.mlp.down_proj.qzeros', 'model.layers.27.mlp.down_proj.scales', 'model.layers.27.mlp.gate_proj.qweight', 'model.layers.27.mlp.gate_proj.qzeros', 'model.layers.27.mlp.gate_proj.scales', 'model.layers.27.mlp.up_proj.qweight', 'model.layers.27.mlp.up_proj.qzeros', 'model.layers.27.mlp.up_proj.scales', 'model.layers.27.self_attn.k_proj.qweight', 'model.layers.27.self_attn.k_proj.qzeros', 'model.layers.27.self_attn.k_proj.scales', 'model.layers.27.self_attn.o_proj.qweight', 'model.layers.27.self_attn.o_proj.qzeros', 'model.layers.27.self_attn.o_proj.scales', 'model.layers.27.self_attn.q_proj.qweight', 'model.layers.27.self_attn.q_proj.qzeros', 'model.layers.27.self_attn.q_proj.scales', 'model.layers.27.self_attn.v_proj.qweight', 'model.layers.27.self_attn.v_proj.qzeros', 'model.layers.27.self_attn.v_proj.scales', 'model.layers.28.mlp.down_proj.qweight', 'model.layers.28.mlp.down_proj.qzeros', 'model.layers.28.mlp.down_proj.scales', 'model.layers.28.mlp.gate_proj.qweight', 'model.layers.28.mlp.gate_proj.qzeros', 'model.layers.28.mlp.gate_proj.scales', 'model.layers.28.mlp.up_proj.qweight', 'model.layers.28.mlp.up_proj.qzeros', 'model.layers.28.mlp.up_proj.scales', 'model.layers.28.self_attn.k_proj.qweight', 'model.layers.28.self_attn.k_proj.qzeros', 'model.layers.28.self_attn.k_proj.scales', 'model.layers.28.self_attn.o_proj.qweight', 'model.layers.28.self_attn.o_proj.qzeros', 'model.layers.28.self_attn.o_proj.scales', 'model.layers.28.self_attn.q_proj.qweight', 'model.layers.28.self_attn.q_proj.qzeros', 'model.layers.28.self_attn.q_proj.scales', 'model.layers.28.self_attn.v_proj.qweight', 'model.layers.28.self_attn.v_proj.qzeros', 'model.layers.28.self_attn.v_proj.scales', 'model.layers.29.mlp.down_proj.qweight', 'model.layers.29.mlp.down_proj.qzeros', 'model.layers.29.mlp.down_proj.scales', 'model.layers.29.mlp.gate_proj.qweight', 'model.layers.29.mlp.gate_proj.qzeros', 'model.layers.29.mlp.gate_proj.scales', 'model.layers.29.mlp.up_proj.qweight', 'model.layers.29.mlp.up_proj.qzeros', 'model.layers.29.mlp.up_proj.scales', 'model.layers.29.self_attn.k_proj.qweight', 'model.layers.29.self_attn.k_proj.qzeros', 'model.layers.29.self_attn.k_proj.scales', 'model.layers.29.self_attn.o_proj.qweight', 'model.layers.29.self_attn.o_proj.qzeros', 'model.layers.29.self_attn.o_proj.scales', 'model.layers.29.self_attn.q_proj.qweight', 'model.layers.29.self_attn.q_proj.qzeros', 'model.layers.29.self_attn.q_proj.scales', 'model.layers.29.self_attn.v_proj.qweight', 'model.layers.29.self_attn.v_proj.qzeros', 'model.layers.29.self_attn.v_proj.scales', 'model.layers.3.mlp.down_proj.qweight', 'model.layers.3.mlp.down_proj.qzeros', 'model.layers.3.mlp.down_proj.scales', 'model.layers.3.mlp.gate_proj.qweight', 'model.layers.3.mlp.gate_proj.qzeros', 'model.layers.3.mlp.gate_proj.scales', 'model.layers.3.mlp.up_proj.qweight', 'model.layers.3.mlp.up_proj.qzeros', 'model.layers.3.mlp.up_proj.scales', 'model.layers.3.self_attn.k_proj.qweight', 'model.layers.3.self_attn.k_proj.qzeros', 'model.layers.3.self_attn.k_proj.scales', 'model.layers.3.self_attn.o_proj.qweight', 'model.layers.3.self_attn.o_proj.qzeros', 'model.layers.3.self_attn.o_proj.scales', 'model.layers.3.self_attn.q_proj.qweight', 'model.layers.3.self_attn.q_proj.qzeros', 'model.layers.3.self_attn.q_proj.scales', 'model.layers.3.self_attn.v_proj.qweight', 'model.layers.3.self_attn.v_proj.qzeros', 'model.layers.3.self_attn.v_proj.scales', 'model.layers.30.mlp.down_proj.qweight', 'model.layers.30.mlp.down_proj.qzeros', 'model.layers.30.mlp.down_proj.scales', 'model.layers.30.mlp.gate_proj.qweight', 'model.layers.30.mlp.gate_proj.qzeros', 'model.layers.30.mlp.gate_proj.scales', 'model.layers.30.mlp.up_proj.qweight', 'model.layers.30.mlp.up_proj.qzeros', 'model.layers.30.mlp.up_proj.scales', 'model.layers.30.self_attn.k_proj.qweight', 'model.layers.30.self_attn.k_proj.qzeros', 'model.layers.30.self_attn.k_proj.scales', 'model.layers.30.self_attn.o_proj.qweight', 'model.layers.30.self_attn.o_proj.qzeros', 'model.layers.30.self_attn.o_proj.scales', 'model.layers.30.self_attn.q_proj.qweight', 'model.layers.30.self_attn.q_proj.qzeros', 'model.layers.30.self_attn.q_proj.scales', 'model.layers.30.self_attn.v_proj.qweight', 'model.layers.30.self_attn.v_proj.qzeros', 'model.layers.30.self_attn.v_proj.scales', 'model.layers.31.mlp.down_proj.qweight', 'model.layers.31.mlp.down_proj.qzeros', 'model.layers.31.mlp.down_proj.scales', 'model.layers.31.mlp.gate_proj.qweight', 'model.layers.31.mlp.gate_proj.qzeros', 'model.layers.31.mlp.gate_proj.scales', 'model.layers.31.mlp.up_proj.qweight', 'model.layers.31.mlp.up_proj.qzeros', 'model.layers.31.mlp.up_proj.scales', 'model.layers.31.self_attn.k_proj.qweight', 'model.layers.31.self_attn.k_proj.qzeros', 'model.layers.31.self_attn.k_proj.scales', 'model.layers.31.self_attn.o_proj.qweight', 'model.layers.31.self_attn.o_proj.qzeros', 'model.layers.31.self_attn.o_proj.scales', 'model.layers.31.self_attn.q_proj.qweight', 'model.layers.31.self_attn.q_proj.qzeros', 'model.layers.31.self_attn.q_proj.scales', 'model.layers.31.self_attn.v_proj.qweight', 'model.layers.31.self_attn.v_proj.qzeros', 'model.layers.31.self_attn.v_proj.scales', 'model.layers.4.mlp.down_proj.qweight', 'model.layers.4.mlp.down_proj.qzeros', 'model.layers.4.mlp.down_proj.scales', 'model.layers.4.mlp.gate_proj.qweight', 'model.layers.4.mlp.gate_proj.qzeros', 'model.layers.4.mlp.gate_proj.scales', 'model.layers.4.mlp.up_proj.qweight', 'model.layers.4.mlp.up_proj.qzeros', 'model.layers.4.mlp.up_proj.scales', 'model.layers.4.self_attn.k_proj.qweight', 'model.layers.4.self_attn.k_proj.qzeros', 'model.layers.4.self_attn.k_proj.scales', 'model.layers.4.self_attn.o_proj.qweight', 'model.layers.4.self_attn.o_proj.qzeros', 'model.layers.4.self_attn.o_proj.scales', 'model.layers.4.self_attn.q_proj.qweight', 'model.layers.4.self_attn.q_proj.qzeros', 'model.layers.4.self_attn.q_proj.scales', 'model.layers.4.self_attn.v_proj.qweight', 'model.layers.4.self_attn.v_proj.qzeros', 'model.layers.4.self_attn.v_proj.scales', 'model.layers.5.mlp.down_proj.qweight', 'model.layers.5.mlp.down_proj.qzeros', 'model.layers.5.mlp.down_proj.scales', 'model.layers.5.mlp.gate_proj.qweight', 'model.layers.5.mlp.gate_proj.qzeros', 'model.layers.5.mlp.gate_proj.scales', 'model.layers.5.mlp.up_proj.qweight', 'model.layers.5.mlp.up_proj.qzeros', 'model.layers.5.mlp.up_proj.scales', 'model.layers.5.self_attn.k_proj.qweight', 'model.layers.5.self_attn.k_proj.qzeros', 'model.layers.5.self_attn.k_proj.scales', 'model.layers.5.self_attn.o_proj.qweight', 'model.layers.5.self_attn.o_proj.qzeros', 'model.layers.5.self_attn.o_proj.scales', 'model.layers.5.self_attn.q_proj.qweight', 'model.layers.5.self_attn.q_proj.qzeros', 'model.layers.5.self_attn.q_proj.scales', 'model.layers.5.self_attn.v_proj.qweight', 'model.layers.5.self_attn.v_proj.qzeros', 'model.layers.5.self_attn.v_proj.scales', 'model.layers.6.mlp.down_proj.qweight', 'model.layers.6.mlp.down_proj.qzeros', 'model.layers.6.mlp.down_proj.scales', 'model.layers.6.mlp.gate_proj.qweight', 'model.layers.6.mlp.gate_proj.qzeros', 'model.layers.6.mlp.gate_proj.scales', 'model.layers.6.mlp.up_proj.qweight', 'model.layers.6.mlp.up_proj.qzeros', 'model.layers.6.mlp.up_proj.scales', 'model.layers.6.self_attn.k_proj.qweight', 'model.layers.6.self_attn.k_proj.qzeros', 'model.layers.6.self_attn.k_proj.scales', 'model.layers.6.self_attn.o_proj.qweight', 'model.layers.6.self_attn.o_proj.qzeros', 'model.layers.6.self_attn.o_proj.scales', 'model.layers.6.self_attn.q_proj.qweight', 'model.layers.6.self_attn.q_proj.qzeros', 'model.layers.6.self_attn.q_proj.scales', 'model.layers.6.self_attn.v_proj.qweight', 'model.layers.6.self_attn.v_proj.qzeros', 'model.layers.6.self_attn.v_proj.scales', 'model.layers.7.mlp.down_proj.qweight', 'model.layers.7.mlp.down_proj.qzeros', 'model.layers.7.mlp.down_proj.scales', 'model.layers.7.mlp.gate_proj.qweight', 'model.layers.7.mlp.gate_proj.qzeros', 'model.layers.7.mlp.gate_proj.scales', 'model.layers.7.mlp.up_proj.qweight', 'model.layers.7.mlp.up_proj.qzeros', 'model.layers.7.mlp.up_proj.scales', 'model.layers.7.self_attn.k_proj.qweight', 'model.layers.7.self_attn.k_proj.qzeros', 'model.layers.7.self_attn.k_proj.scales', 'model.layers.7.self_attn.o_proj.qweight', 'model.layers.7.self_attn.o_proj.qzeros', 'model.layers.7.self_attn.o_proj.scales', 'model.layers.7.self_attn.q_proj.qweight', 'model.layers.7.self_attn.q_proj.qzeros', 'model.layers.7.self_attn.q_proj.scales', 'model.layers.7.self_attn.v_proj.qweight', 'model.layers.7.self_attn.v_proj.qzeros', 'model.layers.7.self_attn.v_proj.scales', 'model.layers.8.mlp.down_proj.qweight', 'model.layers.8.mlp.down_proj.qzeros', 'model.layers.8.mlp.down_proj.scales', 'model.layers.8.mlp.gate_proj.qweight', 'model.layers.8.mlp.gate_proj.qzeros', 'model.layers.8.mlp.gate_proj.scales', 'model.layers.8.mlp.up_proj.qweight', 'model.layers.8.mlp.up_proj.qzeros', 'model.layers.8.mlp.up_proj.scales', 'model.layers.8.self_attn.k_proj.qweight', 'model.layers.8.self_attn.k_proj.qzeros', 'model.layers.8.self_attn.k_proj.scales', 'model.layers.8.self_attn.o_proj.qweight', 'model.layers.8.self_attn.o_proj.qzeros', 'model.layers.8.self_attn.o_proj.scales', 'model.layers.8.self_attn.q_proj.qweight', 'model.layers.8.self_attn.q_proj.qzeros', 'model.layers.8.self_attn.q_proj.scales', 'model.layers.8.self_attn.v_proj.qweight', 'model.layers.8.self_attn.v_proj.qzeros', 'model.layers.8.self_attn.v_proj.scales', 'model.layers.9.mlp.down_proj.qweight', 'model.layers.9.mlp.down_proj.qzeros', 'model.layers.9.mlp.down_proj.scales', 'model.layers.9.mlp.gate_proj.qweight', 'model.layers.9.mlp.gate_proj.qzeros', 'model.layers.9.mlp.gate_proj.scales', 'model.layers.9.mlp.up_proj.qweight', 'model.layers.9.mlp.up_proj.qzeros', 'model.layers.9.mlp.up_proj.scales', 'model.layers.9.self_attn.k_proj.qweight', 'model.layers.9.self_attn.k_proj.qzeros', 'model.layers.9.self_attn.k_proj.scales', 'model.layers.9.self_attn.o_proj.qweight', 'model.layers.9.self_attn.o_proj.qzeros', 'model.layers.9.self_attn.o_proj.scales', 'model.layers.9.self_attn.q_proj.qweight', 'model.layers.9.self_attn.q_proj.qzeros', 'model.layers.9.self_attn.q_proj.scales', 'model.layers.9.self_attn.v_proj.qweight', 'model.layers.9.self_attn.v_proj.qzeros', 'model.layers.9.self_attn.v_proj.scales']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(quant_path,trust_remote_code=True).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(quant_path,trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    print(inputs)\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    print(out)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1, 67536, 71478]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}\n",
      "tensor([[    1, 67536, 71478,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'介绍你自己'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('介绍你自己')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
