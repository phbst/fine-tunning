{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[## 文档链接](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ：面向预训练 Transformer 模型设计的量化技术（ICLR 2023）\n",
    "GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers 是一个高效、精准的\n",
    "量化技术，特别适用于大规模GPT模型，能够在显著降低模型大小和计算需求的同时，保持高准确度和推理速度。\n",
    "***\n",
    "GPTQ算法具有以下技术特点：\\\n",
    "\\\n",
    "1.专为GPT模型设计：GPTQ针对大规模GPT模型（如1750亿参数规模的模型）进行优化，解决了这类模型因\n",
    "规模庞大导致的高计算和存储成本问题。\\\n",
    "2.一次性权重量化方法：GPTQ是一种基于近似二阶信息的权重量化方法，能够在一次处理中完成模型的量化。\\\n",
    "3.高效率：GPTQ能在大约四个GPU小时内完成1750亿参数的GPT模型的量化。\\\n",
    "4.低位宽量化：通过将权重位宽降至每个权重3或4位，GPTQ显著减少了模型的大小。\\\n",
    "5.准确度保持：即便在进行显著的位宽减少后，GPTQ也能保持与未压缩模型相近的准确度，减少性能损失。\\\n",
    "6.支持极端量化：GPTQ还可以实现更极端的量化，如2位或三元量化，同时保持合理的准确度。\\\n",
    "7.推理速度提升：使用GPTQ量化的模型在高端GPU（如NVIDIA A100）上实现了大约3.25倍的推理速度提升，\n",
    "在成本效益更高的GPU（如NVIDIA A6000）上实现了大约4.5倍的速度提升。\\\n",
    "8.适用于单GPU环境：GPTQ使得在单个GPU内执行大规模模型的生成推理成为可能，显著降低了部署这类模\n",
    "型的硬件要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304084405.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ 量化算法核心流程\n",
    "核心步骤：使用存储在Cholesky（切尔斯基）分解中的逆Hessian（海森）\n",
    "信息量化连续列的块（加粗表示），并在步骤结束时更新剩余的权重\n",
    "（蓝色表示），在每个块内递归（白色中间块）地应用量化过程。\\\n",
    "GPTQ量化过程的关键步骤操作，具体描述如下：\\\n",
    "1.块量化：选择一块连续的列（在图中加粗表示），并将其作为当前步骤\n",
    "的量化目标。\\\n",
    "2.使用Cholesky分解：利用Cholesky分解得到的逆Hessian信息来量化选定的块。Cholesky分解提供了一种数值稳定的方法来处理逆矩阵，这对于维\n",
    "持量化过程的准确性至关重要。\\\n",
    "3.权重更新：在每个量化步骤的最后，更新剩余的权重（在图中以蓝色表\n",
    "示）。这个步骤确保了整个量化过程的连贯性和精确性。\\\n",
    "4.递归量化：在每个选定的块内部，量化过程是递归应用的。这意味着量\n",
    "化过程首先聚焦于一个较小的子块，然后逐步扩展到整个块。\n",
    "通过这种方式，GPTQ方法能够在保持高度精度的同时，高效地处理大量\n",
    "的权重，这对于大型模型的量化至关重要。这种策略特别适用于处理大\n",
    "型、复杂的模型，如GPT系列，其中权重数量巨大，且量化过程需要特别\n",
    "小心以避免精度损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活感知权重量化（Activation-aware Weight Quantization, AWQ）\n",
    "激活感知权重量化（AWQ）算法，其原理不是对模型中的所有权重进行量化，而是仅保留小部分（1%）对LLM性能\\\n",
    "至关重要的权重。其算法主要特点如下：\\\n",
    "1.低位权重量化：AWQ专为大型语言模型（LLMs）设计，支持低位（即少位数）的权重量化，有效减少模型大小。\\\n",
    "2.重点保护显著权重：AWQ基于权重重要性不均的观察，只需保护大约1%的显著权重，即可显著减少量化误差。\\\n",
    "3.观察激活而非权重：在确定哪些权重是显著的过程中，AWQ通过观察激活分布而非权重分布来进行。\\\n",
    "4.无需反向传播或重构：AWQ不依赖于复杂的反向传播或重构过程，因此能够更好地保持模型的泛化能力，避免对\n",
    "特定数据集的过拟合。\\\n",
    "5.适用于多种模型和任务：AWQ在多种语言建模任务和领域特定基准测试中表现出色，包括指令调整的语言模型和\n",
    "多模态语言模型。\\\n",
    "6.高效的推理框架：与AWQ配套的是一个为LLMs量身定做的高效推理框架，提供显著的速度提升，适用于桌面和\n",
    "移动GPU。\\\n",
    "7.支持边缘设备部署：这种方法支持在内存和计算能力有限的边缘设备（如NVIDIA Jetson Orin 64GB）上部署大\n",
    "型模型，如70B Llama-2模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304092755.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304093236.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitsAndBytes 简介\n",
    "BitsAndBytes（BNB）是自定义CUDA函数的轻量级包装器，特别是8比特优化器、矩阵乘法和量\n",
    "化函数。主要特征如下：\\\n",
    " \\\n",
    "•具有混合精度分解的8比特矩阵乘法\\\n",
    "•LLM.int8()推理\\\n",
    "•8比特优化器：Adam、AdamW、RMSProp、LARS、LAMB、Lion（节省75%内存）\\\n",
    "•稳定的嵌入层：通过更好的初始化和标准化提高稳定性\\\n",
    "•8比特量化：分位数、线性和动态量化\\\n",
    "•快速的分位数估计：比其他算法快100倍\\\n",
    " \\\n",
    "在 Transformers 量化方案中，BNB 是将模型量化为8位和4位的最简单选择。\\\n",
    " \\\n",
    "•8位量化将fp16中的异常值与int8中的非异常值相乘，将非异常值转换回fp16，然后将它们相加以\n",
    "返回fp16中的权重。这减少了异常值对模型性能产生的降级效果。\\\n",
    "•4位量化进一步压缩了模型，并且通常与QLoRA一起用于微调量化LLM（低精度语言模型）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained('E:\\model\\language\\opt-125m',trust_remote_code=True,device_map='auto').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结 AWQ yyds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码环节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240305102346.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#配置\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,AwqConfig\n",
    "from awq import AutoAWQForCausalLM\n",
    "import torch\n",
    "model_path='E:\\model\\language\\opt-125m'\n",
    "quant_path='E:\\model\\language\\quant\\opt-125m-awq'\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path,trust_remote_code=True, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "c:\\Users\\86195\\.conda\\envs\\transformers\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "AWQ: 100%|██████████| 12/12 [01:57<00:00,  9.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型配置\n",
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_method': <QuantizationMethod.AWQ: 'awq'>,\n",
       " 'bits': 4,\n",
       " 'group_size': 128,\n",
       " 'zero_point': True,\n",
       " 'version': <AWQLinearVersion.GEMM: 'gemm'>,\n",
       " 'backend': <AwqBackendPackingMethod.AUTOAWQ: 'autoawq'>,\n",
       " 'fuse_max_seq_len': None,\n",
       " 'modules_to_not_convert': None,\n",
       " 'modules_to_fuse': None,\n",
       " 'do_fuse': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_method': <QuantizationMethod.AWQ: 'awq'>,\n",
       " 'bits': 4,\n",
       " 'group_size': 128,\n",
       " 'zero_point': True,\n",
       " 'version': <AWQLinearVersion.GEMM: 'gemm'>,\n",
       " 'backend': <AwqBackendPackingMethod.AUTOAWQ: 'autoawq'>,\n",
       " 'fuse_max_seq_len': None,\n",
       " 'modules_to_not_convert': None,\n",
       " 'modules_to_fuse': None,\n",
       " 'do_fuse': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.config.quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('E:\\\\model\\\\language\\\\quant\\\\opt-125m-awq\\\\tokenizer_config.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-awq\\\\special_tokens_map.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-awq\\\\vocab.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-awq\\\\merges.txt',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-awq\\\\added_tokens.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-awq\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of OptAWQForCausalLM(\n",
       "  (model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): WQLinear_GEMM(in_features=768, out_features=768, bias=True, w_bit=4, group_size=128)\n",
       "              (v_proj): WQLinear_GEMM(in_features=768, out_features=768, bias=True, w_bit=4, group_size=128)\n",
       "              (q_proj): WQLinear_GEMM(in_features=768, out_features=768, bias=True, w_bit=4, group_size=128)\n",
       "              (out_proj): WQLinear_GEMM(in_features=768, out_features=768, bias=True, w_bit=4, group_size=128)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): WQLinear_GEMM(in_features=768, out_features=3072, bias=True, w_bit=4, group_size=128)\n",
       "            (fc2): WQLinear_GEMM(in_features=3072, out_features=768, bias=True, w_bit=4, group_size=128)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.model.decoder.embed_tokens.weight data type: torch.float16\n",
      "Parameter model.model.decoder.embed_positions.weight data type: torch.float16\n",
      "Parameter model.model.decoder.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.0.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.0.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.0.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.0.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.1.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.1.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.1.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.1.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.2.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.2.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.2.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.2.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.3.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.3.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.3.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.3.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.4.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.4.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.4.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.4.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.5.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.5.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.5.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.5.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.6.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.6.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.6.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.6.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.7.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.7.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.7.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.7.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.8.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.8.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.8.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.8.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.9.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.9.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.9.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.9.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.10.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.10.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.10.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.10.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.11.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.11.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.model.decoder.layers.11.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.model.decoder.layers.11.final_layer_norm.bias data type: torch.float16\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for name, param in model.named_parameters():\n",
    "    n=n+1\n",
    "    print(f\"Parameter {name} data type: {param.dtype}\")\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240305102351.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,GPTQConfig\n",
    "import torch\n",
    "model_path='E:\\model\\language\\opt-125m'\n",
    "quant_path='E:\\model\\language\\quant\\opt-125m-gptq'\n",
    "quant_config=GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    dataset=[\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"],\n",
    "    desc_act=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2412425a46b24ab79c645094ae970cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.decoder.layers blocks :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18223db9254f412db0dc5e9dfa9918c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a78feab29443179753139fa33f27d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65188320a64d4400b1444ee8dd483cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445a76f0b4e5411fa883673a4f37d3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2845a8fbc446491fa4ab0fadebcc69f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1858d3be23e40308d3cc6972956f9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b8ea9f0354c418b622d59b9a7158c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dee97f0f6e4860ae7f66db7441447a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e627b72679af486bb49ece5f6ae1d9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18693fb4502046bfa64d19b32e5058e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e4f78b293e4274ab39d7df68c049e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d849e9195e8484ab1a4c646b16871e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_path,trust_remote_code=True)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_path,quantization_config=quant_config, torch_dtype=torch.float16, device_map=\"auto\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict([('qweight',\n",
       "               tensor([[ 1712808666, -1248295259, -2025411892,  ..., -1486452502,\n",
       "                         2019142072, -1735820810],\n",
       "                       [-2000132747,  -578262345,  1484081337,  ..., -1230600537,\n",
       "                        -2019252056, -2023311003],\n",
       "                       [ -710293850, -1153090188,  1431922298,  ..., -1768449094,\n",
       "                        -1984337253,  2022406582],\n",
       "                       ...,\n",
       "                       [-1451935592, -1494580055, -1772844344,  ..., -1517635426,\n",
       "                         -664417400,  -409622870],\n",
       "                       [-2007473565,  1218733898,  1737251004,  ...,  1741199510,\n",
       "                        -1732560249, -1754850968],\n",
       "                       [ 1999202918, -1986294939,  1737140825,  ..., -1461086871,\n",
       "                        -1450465416, -1756087955]], device='cuda:0', dtype=torch.int32)),\n",
       "              ('qzeros',\n",
       "               tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "                      device='cuda:0', dtype=torch.int32)),\n",
       "              ('scales',\n",
       "               tensor([[0.0472, 0.0420, 0.0370,  ..., 0.0180, 0.0131, 0.0191],\n",
       "                       [0.0509, 0.0485, 0.0372,  ..., 0.0105, 0.0142, 0.0115],\n",
       "                       [0.0425, 0.0530, 0.0417,  ..., 0.0190, 0.0141, 0.0219],\n",
       "                       [0.0486, 0.0345, 0.0442,  ..., 0.0122, 0.0151, 0.0115],\n",
       "                       [0.0418, 0.0492, 0.0381,  ..., 0.0196, 0.0169, 0.0148],\n",
       "                       [0.0470, 0.0479, 0.0373,  ..., 0.0139, 0.0147, 0.0164]],\n",
       "                      device='cuda:0', dtype=torch.float16)),\n",
       "              ('g_idx',\n",
       "               tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "                      device='cuda:0', dtype=torch.int32)),\n",
       "              ('bias',\n",
       "               tensor([-1.4294e-01, -3.6646e-01, -2.9221e-02, -2.8613e-01, -3.0615e-01,\n",
       "                       -2.6880e-01,  7.9773e-02,  8.0261e-02, -1.7957e-01, -5.2887e-02,\n",
       "                        1.4319e-01, -1.8982e-01,  3.6694e-01, -6.1554e-02,  3.2617e-01,\n",
       "                       -2.8857e-01,  6.8481e-02, -2.7271e-01,  1.5649e-01, -2.1838e-01,\n",
       "                       -2.1082e-01, -1.3550e-01, -2.5049e-01, -3.3594e-01,  2.9150e-01,\n",
       "                       -7.9773e-02,  1.3440e-01,  8.8196e-02,  3.8110e-01,  1.4294e-01,\n",
       "                       -7.5989e-02, -1.9873e-01, -2.1973e-02,  1.3397e-02,  1.1505e-01,\n",
       "                        2.5146e-01,  2.8589e-01,  2.9956e-01, -2.2217e-01,  1.3135e-01,\n",
       "                       -2.5269e-01, -2.0190e-01,  8.5083e-02, -5.4901e-02,  2.3267e-01,\n",
       "                       -1.3831e-01, -5.8624e-02, -1.1206e-01,  5.0488e-01,  1.4526e-01,\n",
       "                        5.2295e-01,  2.3157e-01, -1.0779e-01,  2.0374e-01,  1.8323e-01,\n",
       "                       -3.3862e-01,  1.9885e-01, -1.6028e-01, -6.7139e-02,  3.3740e-01,\n",
       "                        1.3025e-01,  1.1078e-01,  3.5034e-01, -2.9590e-01,  5.2539e-01,\n",
       "                        2.8003e-01, -3.3691e-02,  3.4546e-01,  1.3342e-01,  1.0510e-01,\n",
       "                       -1.2433e-01,  2.7686e-01, -4.4098e-02,  6.0921e-03,  1.0352e-01,\n",
       "                        5.0635e-01, -1.7041e-01,  5.0391e-01, -3.5010e-01,  5.4590e-01,\n",
       "                       -1.5039e-01, -1.8030e-01, -1.4600e-01,  3.5498e-01,  1.1182e-01,\n",
       "                        2.5659e-01, -3.5352e-01, -3.8379e-01,  5.3174e-01, -2.0215e-01,\n",
       "                       -2.5928e-01, -1.6821e-01,  3.0304e-02, -2.7026e-01, -4.4312e-01,\n",
       "                       -2.2449e-01,  1.2000e-01,  6.0059e-02,  2.6953e-01,  3.9722e-01,\n",
       "                       -5.0195e-01, -4.9927e-02,  6.1920e-02, -1.1469e-01, -1.8988e-03,\n",
       "                       -6.6650e-02, -1.6272e-01,  3.7134e-01, -3.2568e-01,  7.4196e-03,\n",
       "                        6.5079e-03, -3.2318e-02, -2.0874e-02, -3.4790e-01, -4.3579e-02,\n",
       "                        3.3618e-01, -1.6418e-01, -4.5441e-02,  2.7246e-01,  1.1267e-01,\n",
       "                       -3.1738e-01, -9.1919e-02, -3.3008e-01,  4.5532e-02,  2.4094e-02,\n",
       "                       -2.0227e-01, -1.8384e-01,  9.4910e-02, -5.8167e-02, -2.6343e-01,\n",
       "                       -1.4722e-01,  3.2153e-01, -8.6609e-02, -3.0957e-01,  2.8174e-01,\n",
       "                       -2.3413e-01,  1.7090e-01,  2.5317e-01,  1.2683e-01, -3.0444e-01,\n",
       "                        2.6855e-01,  3.8849e-02,  2.2925e-01,  1.9922e-01,  2.4719e-01,\n",
       "                        4.0942e-01,  1.1780e-01,  1.1475e-01, -6.1127e-02,  2.1313e-01,\n",
       "                        3.8184e-01, -5.8899e-02,  1.0278e-01,  2.3950e-01,  2.8784e-01,\n",
       "                       -3.0457e-02, -1.3049e-01, -3.7646e-01,  2.2119e-01, -1.6406e-01,\n",
       "                       -1.9495e-01,  1.7273e-01, -2.5464e-01,  3.6768e-01, -2.9102e-01,\n",
       "                       -1.5918e-01,  2.0471e-01, -3.2812e-01,  2.2131e-01, -1.1847e-01,\n",
       "                        2.0642e-01,  3.2324e-01, -4.1895e-01, -1.8042e-01, -2.6733e-01,\n",
       "                       -2.9834e-01, -2.4695e-01, -2.6465e-01,  2.0676e-02,  6.9763e-02,\n",
       "                        1.1255e-01,  3.0957e-01,  5.0293e-01,  3.1421e-01,  1.4185e-01,\n",
       "                       -2.6562e-01, -2.4817e-01, -5.6854e-02,  1.8518e-01, -2.5269e-01,\n",
       "                       -2.2961e-01, -8.8867e-02, -4.0796e-01, -1.9324e-01,  4.3262e-01,\n",
       "                       -3.6523e-01,  4.0016e-03,  1.8518e-01,  2.6416e-01,  2.4414e-01,\n",
       "                        9.4177e-02, -1.8005e-01, -1.4246e-01, -2.9395e-01,  3.0396e-01,\n",
       "                       -3.4131e-01, -4.0039e-01, -2.5732e-01, -3.2959e-01,  1.6553e-01,\n",
       "                        2.8491e-01, -2.8394e-01,  5.0342e-01,  2.0190e-01, -4.0576e-01,\n",
       "                        1.8066e-01,  2.8882e-01,  4.3945e-01,  7.6561e-03, -3.0322e-01,\n",
       "                        2.8101e-01,  1.8518e-01, -3.6682e-02,  1.5088e-01,  2.3340e-01,\n",
       "                       -4.3042e-01, -3.6475e-01, -3.6640e-03, -3.4912e-01, -3.8013e-01,\n",
       "                       -4.0436e-02,  3.5474e-01, -4.4434e-01,  5.0830e-01,  3.2373e-01,\n",
       "                        2.6807e-01,  2.6416e-01, -4.2505e-01,  2.9956e-01,  2.4902e-01,\n",
       "                        2.5269e-01,  3.7646e-01, -4.3091e-01,  3.6304e-01,  1.9238e-01,\n",
       "                        1.1406e-02, -9.9258e-03, -2.1826e-01, -4.3396e-02, -3.0566e-01,\n",
       "                       -3.7842e-01, -1.7554e-01, -5.0293e-01, -3.7109e-01,  3.6548e-01,\n",
       "                       -4.4824e-01,  3.2654e-02, -1.1391e-02,  6.0730e-02,  1.7426e-02,\n",
       "                       -1.6431e-01, -8.7219e-02,  1.8884e-01,  7.8552e-02, -4.7821e-02,\n",
       "                        9.8724e-03, -1.9165e-02, -9.9945e-03, -1.0797e-01, -4.2084e-02,\n",
       "                        3.9635e-03, -2.5732e-01,  1.3252e-02,  7.1289e-02,  1.6113e-02,\n",
       "                       -1.3382e-02, -2.1393e-02,  8.0872e-02, -2.5952e-01,  2.6932e-02,\n",
       "                       -1.4763e-02, -2.5284e-02, -2.7905e-01, -3.7659e-02, -1.9409e-02,\n",
       "                       -8.9050e-02, -9.7733e-03, -6.6223e-02,  3.5400e-02, -7.4654e-03,\n",
       "                        6.4697e-02,  1.6052e-01, -1.9073e-02, -1.9806e-02,  3.6835e-02,\n",
       "                        6.1737e-02, -1.0675e-01,  1.5793e-02,  1.3147e-01,  2.5772e-02,\n",
       "                        3.2501e-02,  3.2440e-02, -4.0619e-02,  4.0359e-03, -5.0140e-02,\n",
       "                       -7.7087e-02,  1.2772e-02,  7.0862e-02, -5.2246e-02,  5.9509e-04,\n",
       "                       -2.0569e-02, -4.2999e-02,  1.4465e-02, -2.6562e-01,  5.4359e-03,\n",
       "                       -9.1858e-02, -6.3171e-02,  1.6586e-02,  4.0649e-02,  8.2214e-02,\n",
       "                        2.0844e-02,  5.9967e-03,  4.3488e-02,  2.6047e-02, -2.3384e-03,\n",
       "                       -1.2207e-02, -2.7603e-02,  5.6250e-01, -1.6510e-02, -1.4435e-02,\n",
       "                        5.2930e-01,  2.1805e-02, -1.0262e-03, -2.6810e-02, -2.4414e-03,\n",
       "                       -2.5978e-03,  5.5615e-01, -1.9653e-02, -3.8357e-03, -1.4435e-02,\n",
       "                       -1.5656e-02, -5.8441e-02, -1.7746e-02, -2.8870e-02,  4.0314e-02,\n",
       "                        7.3671e-04,  2.7588e-02, -2.4231e-02,  1.5434e-02,  7.2289e-03,\n",
       "                       -1.5221e-02,  3.0075e-02,  1.4343e-02,  1.7609e-02, -7.1869e-03,\n",
       "                       -2.0630e-02, -3.5736e-02,  3.8574e-02,  6.7139e-04,  1.4320e-02,\n",
       "                        4.2343e-03, -3.5156e-02,  2.5558e-02,  4.6659e-04, -1.2123e-02,\n",
       "                       -2.3880e-02,  1.8906e-02,  7.2937e-03, -2.8259e-02,  1.5465e-02,\n",
       "                       -5.1074e-01,  3.4149e-02,  1.1234e-03,  1.4786e-02,  1.7639e-02,\n",
       "                        5.1849e-02, -6.3591e-03,  4.0741e-03, -2.8381e-03, -2.1534e-03,\n",
       "                       -3.2532e-02,  4.7424e-02,  1.7365e-02,  5.9853e-03,  1.7651e-01,\n",
       "                       -6.3477e-02, -3.6133e-02, -1.9006e-01,  2.5586e-01, -3.6938e-01,\n",
       "                       -3.5034e-01, -1.9250e-01, -3.6938e-01, -1.6724e-01,  3.4619e-01,\n",
       "                        2.4377e-01,  1.3721e-01,  1.8158e-02,  1.2109e-01,  3.1128e-01,\n",
       "                       -4.5228e-04,  1.8616e-01,  4.4580e-01,  2.2864e-01, -3.3661e-02,\n",
       "                       -2.1655e-01,  4.4507e-01, -1.2964e-01, -1.9812e-01, -2.3779e-01,\n",
       "                       -2.4524e-01,  2.4585e-01, -2.5439e-01,  4.7095e-01, -6.5308e-02,\n",
       "                       -3.3765e-01, -4.6045e-01, -1.1108e-01,  2.8442e-01,  1.3000e-01,\n",
       "                        5.0342e-01,  1.7236e-01,  1.1676e-01,  1.6870e-01,  3.2983e-01,\n",
       "                       -2.1204e-01,  1.1945e-01,  9.6313e-02,  1.2390e-01, -3.0811e-01,\n",
       "                        3.0981e-01, -7.5623e-02,  1.7664e-01,  3.6572e-01,  4.6582e-01,\n",
       "                        1.7114e-01, -2.4744e-01,  7.0496e-02, -3.2837e-02,  3.2568e-01,\n",
       "                        1.4648e-01, -2.9785e-01, -1.1273e-01,  1.7529e-01, -3.9429e-01,\n",
       "                       -1.6736e-01, -3.0609e-02, -4.4824e-01, -6.6284e-02,  1.2543e-02,\n",
       "                       -1.6614e-01,  2.9614e-01, -3.0859e-01, -1.7639e-01,  8.0322e-02,\n",
       "                       -3.1348e-01, -2.6514e-01, -2.7295e-01, -2.3755e-01,  1.0217e-01,\n",
       "                        8.8379e-02,  8.1177e-02,  3.8391e-02, -5.9509e-02,  3.3325e-01,\n",
       "                       -7.1594e-02, -7.3669e-02,  2.3544e-02,  4.9347e-02,  1.7932e-01,\n",
       "                        1.3123e-01,  1.7517e-01, -3.6499e-01,  2.6733e-01, -2.6245e-01,\n",
       "                       -8.0490e-03,  7.1487e-03, -2.1204e-01,  5.1123e-01, -1.5356e-01,\n",
       "                       -4.1382e-01, -1.7810e-01, -6.2866e-02,  1.1420e-01,  8.2520e-02,\n",
       "                       -3.1641e-01, -1.5335e-02, -5.0995e-02, -2.9199e-01, -1.7847e-01,\n",
       "                        4.2145e-02,  2.9663e-01, -7.2144e-02, -1.3220e-01,  3.0322e-01,\n",
       "                       -1.2683e-01, -2.8961e-02,  2.2430e-02,  7.2876e-02,  7.6599e-02,\n",
       "                       -2.7145e-02,  2.5854e-01, -1.2128e-01, -3.8867e-01, -6.0699e-02,\n",
       "                        2.7786e-02, -3.5059e-01,  6.4636e-02,  1.3660e-01,  2.9395e-01,\n",
       "                       -1.2329e-01, -8.0444e-02, -9.2163e-02, -5.5518e-01,  1.5649e-01,\n",
       "                       -5.0732e-01, -5.2539e-01,  5.5127e-01, -2.2064e-02, -6.2158e-01,\n",
       "                        5.0586e-01, -5.2490e-01,  1.0199e-01, -2.4683e-01,  5.1807e-01,\n",
       "                        3.6499e-01, -5.5615e-01,  5.6299e-01, -1.1469e-01, -5.1172e-01,\n",
       "                        1.5137e-01,  5.5811e-01, -5.7861e-01, -5.0928e-01,  3.2617e-01,\n",
       "                        2.3938e-01, -5.7666e-01,  1.6016e-01, -2.5488e-01, -3.4863e-01,\n",
       "                        2.4780e-01,  3.9337e-02, -1.6858e-01, -8.2825e-02, -3.3325e-02,\n",
       "                        5.0146e-01,  2.9688e-01,  1.3110e-01,  5.7812e-01,  2.0154e-01,\n",
       "                        8.7585e-02, -5.1416e-01, -2.4902e-01, -1.6833e-01,  5.3760e-01,\n",
       "                       -5.3857e-01,  5.6915e-02,  5.0195e-01,  1.3464e-01,  5.0586e-01,\n",
       "                        6.7383e-02,  1.7346e-01, -2.5781e-01, -5.3857e-01,  7.5455e-03,\n",
       "                       -5.0781e-01, -3.9404e-01,  3.0176e-01,  5.0977e-01, -1.2421e-01,\n",
       "                       -5.0293e-01, -1.2347e-01,  5.1416e-01, -5.3320e-01, -5.7080e-01,\n",
       "                       -3.4644e-01,  5.0781e-01, -7.8796e-02, -4.2236e-02, -2.6221e-01,\n",
       "                        1.0114e-01,  1.0971e-02, -7.5562e-02, -2.2595e-01, -1.4502e-01,\n",
       "                        3.7671e-01,  3.6938e-01,  1.5576e-01,  5.8632e-03,  1.6602e-01,\n",
       "                        2.8955e-01, -4.7217e-01, -1.1194e-01,  2.9077e-01,  2.6758e-01,\n",
       "                       -6.3293e-02,  1.1340e-01,  5.3174e-01,  4.0210e-01,  1.3965e-01,\n",
       "                        1.1505e-01,  8.7891e-02, -8.8562e-02, -4.9023e-01,  3.9380e-01,\n",
       "                        1.3611e-01,  2.3767e-01,  1.9424e-02, -5.5322e-01,  2.3218e-01,\n",
       "                       -5.2881e-01, -1.0211e-01,  2.5854e-01, -3.2520e-01,  1.6632e-02,\n",
       "                       -2.7979e-01,  2.8857e-01, -7.1289e-02, -5.2979e-01,  2.5195e-01,\n",
       "                       -3.1226e-01, -2.7466e-03,  1.2793e-01,  9.7168e-02,  6.2103e-02,\n",
       "                        9.3567e-02,  2.4023e-01,  4.6069e-01, -5.0926e-03,  2.7271e-01,\n",
       "                        1.3184e-01, -2.3901e-01, -3.5205e-01, -1.8982e-01,  3.2806e-02,\n",
       "                        1.8774e-01, -1.1591e-01, -5.9021e-02,  2.8702e-02, -3.8379e-01,\n",
       "                        1.9470e-01,  2.2986e-01,  1.3220e-01,  1.9446e-01,  3.7292e-02,\n",
       "                        1.5076e-01, -6.5308e-02, -3.5461e-02, -7.5867e-02,  1.3379e-01,\n",
       "                        1.0284e-01, -2.7740e-02, -4.9324e-03,  1.3000e-01, -9.5032e-02,\n",
       "                        2.3041e-02,  4.3884e-02,  1.4294e-01, -5.5054e-02,  6.8909e-02,\n",
       "                        5.5939e-02,  6.2408e-02, -2.1582e-01,  2.5000e-01,  5.3076e-01,\n",
       "                       -2.2791e-01, -2.5049e-01,  1.6138e-01, -9.7229e-02, -2.1667e-01,\n",
       "                       -2.4976e-01, -2.5146e-01, -5.2643e-02, -1.2457e-01, -7.0229e-03,\n",
       "                       -1.5002e-01, -8.4473e-02, -9.4681e-03,  8.1055e-02,  9.6252e-02,\n",
       "                        1.7773e-01, -8.1604e-02,  1.3806e-01, -1.8677e-01,  1.6895e-01,\n",
       "                       -3.7628e-02,  5.9662e-02, -9.2773e-02, -7.1533e-02, -1.9885e-01,\n",
       "                        5.9174e-02,  2.5000e-01, -2.3514e-02,  7.3608e-02,  2.2290e-01,\n",
       "                        6.6772e-02, -1.2500e-01, -1.3550e-01, -6.0028e-02, -1.3147e-01,\n",
       "                       -1.2756e-01,  6.3416e-02,  1.3062e-01,  1.2488e-01,  6.1584e-02,\n",
       "                       -6.7101e-03, -3.3875e-02, -1.6556e-02,  1.8738e-02, -4.7836e-03,\n",
       "                       -3.2776e-02,  1.4427e-02, -5.1221e-01, -4.6661e-02,  3.5343e-03,\n",
       "                        1.5762e-02,  2.3918e-03, -3.1250e-02, -1.8661e-02, -3.0685e-02,\n",
       "                        1.5762e-02, -8.2779e-03, -7.0190e-02,  1.4915e-02, -1.6663e-02,\n",
       "                        2.6138e-02, -8.9645e-03,  1.2150e-03, -1.5222e-01, -2.7023e-02,\n",
       "                       -2.4277e-02, -8.3237e-03,  2.5192e-02, -7.4120e-03, -7.1487e-03,\n",
       "                       -3.6377e-01,  7.0496e-02, -2.0630e-01,  2.1744e-02,  2.2049e-02,\n",
       "                       -1.7487e-02, -5.1910e-02,  3.1799e-02,  1.8585e-02,  2.3880e-02,\n",
       "                       -1.8433e-02,  9.0866e-03,  1.8753e-02,  1.7273e-02, -4.1580e-03,\n",
       "                        1.5976e-02, -2.8519e-02,  4.3030e-03, -2.9312e-02,  3.5217e-02,\n",
       "                        4.0771e-02, -1.8188e-01,  9.0103e-03,  5.0586e-01,  2.0020e-02,\n",
       "                        1.9806e-02,  1.6861e-02,  2.5436e-02,  1.7548e-02, -8.7357e-03,\n",
       "                       -1.6785e-02,  1.8906e-02, -1.3695e-02], device='cuda:0',\n",
       "                      dtype=torch.float16))]),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'infeatures': 768,\n",
       " 'outfeatures': 768,\n",
       " 'bits': 4,\n",
       " 'group_size': 128,\n",
       " 'maxq': 15,\n",
       " 'half_indim': 384,\n",
       " 'use_cuda_fp16': True,\n",
       " 'wf': tensor([[ 0,  4,  8, 12, 16, 20, 24, 28]], dtype=torch.int32),\n",
       " 'kernel_switch_threshold': 128,\n",
       " 'autogptq_cuda_available': False,\n",
       " 'autogptq_cuda': None,\n",
       " 'trainable': False,\n",
       " 'device': device(type='cuda', index=0)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.decoder.embed_tokens.weight data type: torch.float16\n",
      "Parameter model.decoder.embed_positions.weight data type: torch.float16\n",
      "Parameter model.decoder.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.0.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.0.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.0.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.0.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.1.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.1.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.1.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.1.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.2.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.2.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.2.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.2.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.3.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.3.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.3.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.3.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.4.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.4.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.4.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.4.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.5.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.5.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.5.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.5.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.6.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.6.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.6.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.6.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.7.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.7.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.7.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.7.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.8.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.8.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.8.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.8.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.9.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.9.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.9.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.9.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.10.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.10.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.10.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.10.final_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.11.self_attn_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.11.self_attn_layer_norm.bias data type: torch.float16\n",
      "Parameter model.decoder.layers.11.final_layer_norm.weight data type: torch.float16\n",
      "Parameter model.decoder.layers.11.final_layer_norm.bias data type: torch.float16\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for name, param in model.named_parameters():\n",
    "    n=n+1\n",
    "    print(f\"Parameter {name} data type: {param.dtype}\")\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to be a good to be a good to be a good to be a good to be a good to be a good to be a good to a good to a good to a good to a good to a good to a good to a good to a good to a good to a good to a good to a good to\n"
     ]
    }
   ],
   "source": [
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('E:\\\\model\\\\language\\\\quant\\\\opt-125m-gptq\\\\tokenizer_config.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-gptq\\\\special_tokens_map.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-gptq\\\\vocab.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-gptq\\\\merges.txt',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-gptq\\\\added_tokens.json',\n",
       " 'E:\\\\model\\\\language\\\\quant\\\\opt-125m-gptq\\\\tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240305102356.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "\n",
    "_compute_dtype_map = {\n",
    "    'fp32': torch.float32,\n",
    "    'fp16': torch.float16,\n",
    "    'bf16': torch.bfloat16\n",
    "}\n",
    "\n",
    "# QLoRA 量化配置\n",
    "q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                              bnb_4bit_quant_type='nf4',\n",
    "                              bnb_4bit_use_double_quant=True,\n",
    "                              bnb_4bit_compute_dtype=_compute_dtype_map['bf16'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(model_name_or_path,\n",
    "                                  quantization_config=q_config,\n",
    "                                  device_map='auto',\n",
    "                                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前模型占用的 GPU显存（差值为预留给 PyTorch 的显存）\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)  # 转换为 MiB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,AwqConfig\n",
    "import torch\n",
    "model_path='E:\\model\\language\\opt-125m'\n",
    "quant_path='E:\\model\\language\\quant\\opt-125m-awq'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(quant_path,trust_remote_code=True).cuda()\n",
    "tokenizer=AutoTokenizer.from_pretrained(quant_path,trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    print(inputs)\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    print(out)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 42891]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n",
      "tensor([[    2, 42891,     6,  1437,    38,    95,   300,   103,     9,   127,\n",
      "          1437,  6351,    11,    10,   186,     6,     8,    24,  1326,  5500,\n",
      "             6,    38,   437,  6908,   213,   120,   127,    78,    65,    11,\n",
      "           204,   377,     6,    98,    38,   429,   888,   907,    24, 50118,\n",
      "         13987,    47,   313,   328, 15151,    47,   101,    24,   328,  4832,\n",
      "           495,     2]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"hello,  I just got some of my  gear in a week, and it looks fantastic, I'm gonna go get my first one in 4 months, so I might actually buy it\\nThank you man! Glad you like it! :D\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
