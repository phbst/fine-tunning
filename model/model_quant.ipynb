{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[## 文档链接](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ：面向预训练 Transformer 模型设计的量化技术（ICLR 2023）\n",
    "GPTQ：Accurate Post-Training Quantization for Generative Pre-trained Transformers 是一个高效、精准的\n",
    "量化技术，特别适用于大规模GPT模型，能够在显著降低模型大小和计算需求的同时，保持高准确度和推理速度。\n",
    "***\n",
    "GPTQ算法具有以下技术特点：\\\n",
    "\\\n",
    "1.专为GPT模型设计：GPTQ针对大规模GPT模型（如1750亿参数规模的模型）进行优化，解决了这类模型因\n",
    "规模庞大导致的高计算和存储成本问题。\\\n",
    "2.一次性权重量化方法：GPTQ是一种基于近似二阶信息的权重量化方法，能够在一次处理中完成模型的量化。\\\n",
    "3.高效率：GPTQ能在大约四个GPU小时内完成1750亿参数的GPT模型的量化。\\\n",
    "4.低位宽量化：通过将权重位宽降至每个权重3或4位，GPTQ显著减少了模型的大小。\\\n",
    "5.准确度保持：即便在进行显著的位宽减少后，GPTQ也能保持与未压缩模型相近的准确度，减少性能损失。\\\n",
    "6.支持极端量化：GPTQ还可以实现更极端的量化，如2位或三元量化，同时保持合理的准确度。\\\n",
    "7.推理速度提升：使用GPTQ量化的模型在高端GPU（如NVIDIA A100）上实现了大约3.25倍的推理速度提升，\n",
    "在成本效益更高的GPU（如NVIDIA A6000）上实现了大约4.5倍的速度提升。\\\n",
    "8.适用于单GPU环境：GPTQ使得在单个GPU内执行大规模模型的生成推理成为可能，显著降低了部署这类模\n",
    "型的硬件要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304084405.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ 量化算法核心流程\n",
    "核心步骤：使用存储在Cholesky（切尔斯基）分解中的逆Hessian（海森）\n",
    "信息量化连续列的块（加粗表示），并在步骤结束时更新剩余的权重\n",
    "（蓝色表示），在每个块内递归（白色中间块）地应用量化过程。\\\n",
    "GPTQ量化过程的关键步骤操作，具体描述如下：\\\n",
    "1.块量化：选择一块连续的列（在图中加粗表示），并将其作为当前步骤\n",
    "的量化目标。\\\n",
    "2.使用Cholesky分解：利用Cholesky分解得到的逆Hessian信息来量化选定的块。Cholesky分解提供了一种数值稳定的方法来处理逆矩阵，这对于维\n",
    "持量化过程的准确性至关重要。\\\n",
    "3.权重更新：在每个量化步骤的最后，更新剩余的权重（在图中以蓝色表\n",
    "示）。这个步骤确保了整个量化过程的连贯性和精确性。\\\n",
    "4.递归量化：在每个选定的块内部，量化过程是递归应用的。这意味着量\n",
    "化过程首先聚焦于一个较小的子块，然后逐步扩展到整个块。\n",
    "通过这种方式，GPTQ方法能够在保持高度精度的同时，高效地处理大量\n",
    "的权重，这对于大型模型的量化至关重要。这种策略特别适用于处理大\n",
    "型、复杂的模型，如GPT系列，其中权重数量巨大，且量化过程需要特别\n",
    "小心以避免精度损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活感知权重量化（Activation-aware Weight Quantization, AWQ）\n",
    "激活感知权重量化（AWQ）算法，其原理不是对模型中的所有权重进行量化，而是仅保留小部分（1%）对LLM性能\\\n",
    "至关重要的权重。其算法主要特点如下：\\\n",
    "1.低位权重量化：AWQ专为大型语言模型（LLMs）设计，支持低位（即少位数）的权重量化，有效减少模型大小。\\\n",
    "2.重点保护显著权重：AWQ基于权重重要性不均的观察，只需保护大约1%的显著权重，即可显著减少量化误差。\\\n",
    "3.观察激活而非权重：在确定哪些权重是显著的过程中，AWQ通过观察激活分布而非权重分布来进行。\\\n",
    "4.无需反向传播或重构：AWQ不依赖于复杂的反向传播或重构过程，因此能够更好地保持模型的泛化能力，避免对\n",
    "特定数据集的过拟合。\\\n",
    "5.适用于多种模型和任务：AWQ在多种语言建模任务和领域特定基准测试中表现出色，包括指令调整的语言模型和\n",
    "多模态语言模型。\\\n",
    "6.高效的推理框架：与AWQ配套的是一个为LLMs量身定做的高效推理框架，提供显著的速度提升，适用于桌面和\n",
    "移动GPU。\\\n",
    "7.支持边缘设备部署：这种方法支持在内存和计算能力有限的边缘设备（如NVIDIA Jetson Orin 64GB）上部署大\n",
    "型模型，如70B Llama-2模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304092755.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240304093236.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BitsAndBytes 简介\n",
    "BitsAndBytes（BNB）是自定义CUDA函数的轻量级包装器，特别是8比特优化器、矩阵乘法和量\n",
    "化函数。主要特征如下：\\\n",
    " \\\n",
    "•具有混合精度分解的8比特矩阵乘法\\\n",
    "•LLM.int8()推理\\\n",
    "•8比特优化器：Adam、AdamW、RMSProp、LARS、LAMB、Lion（节省75%内存）\\\n",
    "•稳定的嵌入层：通过更好的初始化和标准化提高稳定性\\\n",
    "•8比特量化：分位数、线性和动态量化\\\n",
    "•快速的分位数估计：比其他算法快100倍\\\n",
    " \\\n",
    "在 Transformers 量化方案中，BNB 是将模型量化为8位和4位的最简单选择。\\\n",
    " \\\n",
    "•8位量化将fp16中的异常值与int8中的非异常值相乘，将非异常值转换回fp16，然后将它们相加以\n",
    "返回fp16中的权重。这减少了异常值对模型性能产生的降级效果。\\\n",
    "•4位量化进一步压缩了模型，并且通常与QLoRA一起用于微调量化LLM（低精度语言模型）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained('E:\\model\\language\\opt-125m',trust_remote_code=True,device_map='auto').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结 AWQ yyds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码环节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240305102346.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,AwqConfig\n",
    "import torch\n",
    "model_path='/mnt/data/opt-125m'\n",
    "quant_path='/mnt/phb/quant'\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config=AwqConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    zero_point=True,\n",
    "    version='gemm'\n",
    ").to_dict()\n",
    "model.model.config.quantization_config=quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/phb/quant/tokenizer_config.json',\n",
       " '/mnt/phb/quant/special_tokens_map.json',\n",
       " '/mnt/phb/quant/vocab.json',\n",
       " '/mnt/phb/quant/merges.txt',\n",
       " '/mnt/phb/quant/added_tokens.json',\n",
       " '/mnt/phb/quant/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240305102351.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,GPTQConfig\n",
    "import torch\n",
    "model_path='/mnt/data/opt-125m'\n",
    "quant_path='/mnt/phb/quant'\n",
    "quant_config=GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    dataset=[\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"],\n",
    "    desc_act=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Quantizing model.decoder.layers blocks : 100%|██████████| 12/12 [00:42<00:00,  3.57s/it]\n",
      "Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n",
      "/opt/conda/envs/phb/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_path,trust_remote_code=True)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_path,quantization_config=quant_config,trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict([('qweight',\n",
       "               tensor([[ 1712808666, -1248295259, -2025411892,  ..., -1486452502,\n",
       "                         2019142072, -1735820810],\n",
       "                       [-2000132747,  -578262345,  1484081337,  ..., -1230600537,\n",
       "                        -2019252056, -2023311003],\n",
       "                       [ -710293850, -1153090188,  1431922298,  ..., -1768449094,\n",
       "                        -1984337253,  2022406582],\n",
       "                       ...,\n",
       "                       [-1451935592, -1494580055, -1772844344,  ..., -1517635426,\n",
       "                         -664417400,  -409622870],\n",
       "                       [-2007473565,  1218733898,  1737251004,  ...,  1741199510,\n",
       "                        -1732560249, -1754850968],\n",
       "                       [ 1999202918, -1986294939,  1737140825,  ..., -1461086871,\n",
       "                        -1450465416, -1756087955]], device='cuda:0', dtype=torch.int32)),\n",
       "              ('qzeros',\n",
       "               tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "                        2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "                      device='cuda:0', dtype=torch.int32)),\n",
       "              ('scales',\n",
       "               tensor([[0.0472, 0.0420, 0.0370,  ..., 0.0180, 0.0131, 0.0191],\n",
       "                       [0.0509, 0.0485, 0.0372,  ..., 0.0105, 0.0142, 0.0115],\n",
       "                       [0.0425, 0.0530, 0.0417,  ..., 0.0190, 0.0141, 0.0219],\n",
       "                       [0.0486, 0.0345, 0.0442,  ..., 0.0122, 0.0151, 0.0115],\n",
       "                       [0.0418, 0.0492, 0.0381,  ..., 0.0196, 0.0169, 0.0148],\n",
       "                       [0.0470, 0.0479, 0.0373,  ..., 0.0139, 0.0147, 0.0164]],\n",
       "                      device='cuda:0', dtype=torch.float16)),\n",
       "              ('g_idx',\n",
       "               tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                       3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "                      device='cuda:0', dtype=torch.int32)),\n",
       "              ('bias',\n",
       "               tensor([-1.4294e-01, -3.6646e-01, -2.9221e-02, -2.8613e-01, -3.0615e-01,\n",
       "                       -2.6880e-01,  7.9773e-02,  8.0261e-02, -1.7957e-01, -5.2887e-02,\n",
       "                        1.4319e-01, -1.8982e-01,  3.6694e-01, -6.1554e-02,  3.2617e-01,\n",
       "                       -2.8857e-01,  6.8481e-02, -2.7271e-01,  1.5649e-01, -2.1838e-01,\n",
       "                       -2.1082e-01, -1.3550e-01, -2.5049e-01, -3.3594e-01,  2.9150e-01,\n",
       "                       -7.9773e-02,  1.3440e-01,  8.8196e-02,  3.8110e-01,  1.4294e-01,\n",
       "                       -7.5989e-02, -1.9873e-01, -2.1973e-02,  1.3397e-02,  1.1505e-01,\n",
       "                        2.5146e-01,  2.8589e-01,  2.9956e-01, -2.2217e-01,  1.3135e-01,\n",
       "                       -2.5269e-01, -2.0190e-01,  8.5083e-02, -5.4901e-02,  2.3267e-01,\n",
       "                       -1.3831e-01, -5.8624e-02, -1.1206e-01,  5.0488e-01,  1.4526e-01,\n",
       "                        5.2295e-01,  2.3157e-01, -1.0779e-01,  2.0374e-01,  1.8323e-01,\n",
       "                       -3.3862e-01,  1.9885e-01, -1.6028e-01, -6.7139e-02,  3.3740e-01,\n",
       "                        1.3025e-01,  1.1078e-01,  3.5034e-01, -2.9590e-01,  5.2539e-01,\n",
       "                        2.8003e-01, -3.3691e-02,  3.4546e-01,  1.3342e-01,  1.0510e-01,\n",
       "                       -1.2433e-01,  2.7686e-01, -4.4098e-02,  6.0921e-03,  1.0352e-01,\n",
       "                        5.0635e-01, -1.7041e-01,  5.0391e-01, -3.5010e-01,  5.4590e-01,\n",
       "                       -1.5039e-01, -1.8030e-01, -1.4600e-01,  3.5498e-01,  1.1182e-01,\n",
       "                        2.5659e-01, -3.5352e-01, -3.8379e-01,  5.3174e-01, -2.0215e-01,\n",
       "                       -2.5928e-01, -1.6821e-01,  3.0304e-02, -2.7026e-01, -4.4312e-01,\n",
       "                       -2.2449e-01,  1.2000e-01,  6.0059e-02,  2.6953e-01,  3.9722e-01,\n",
       "                       -5.0195e-01, -4.9927e-02,  6.1920e-02, -1.1469e-01, -1.8988e-03,\n",
       "                       -6.6650e-02, -1.6272e-01,  3.7134e-01, -3.2568e-01,  7.4196e-03,\n",
       "                        6.5079e-03, -3.2318e-02, -2.0874e-02, -3.4790e-01, -4.3579e-02,\n",
       "                        3.3618e-01, -1.6418e-01, -4.5441e-02,  2.7246e-01,  1.1267e-01,\n",
       "                       -3.1738e-01, -9.1919e-02, -3.3008e-01,  4.5532e-02,  2.4094e-02,\n",
       "                       -2.0227e-01, -1.8384e-01,  9.4910e-02, -5.8167e-02, -2.6343e-01,\n",
       "                       -1.4722e-01,  3.2153e-01, -8.6609e-02, -3.0957e-01,  2.8174e-01,\n",
       "                       -2.3413e-01,  1.7090e-01,  2.5317e-01,  1.2683e-01, -3.0444e-01,\n",
       "                        2.6855e-01,  3.8849e-02,  2.2925e-01,  1.9922e-01,  2.4719e-01,\n",
       "                        4.0942e-01,  1.1780e-01,  1.1475e-01, -6.1127e-02,  2.1313e-01,\n",
       "                        3.8184e-01, -5.8899e-02,  1.0278e-01,  2.3950e-01,  2.8784e-01,\n",
       "                       -3.0457e-02, -1.3049e-01, -3.7646e-01,  2.2119e-01, -1.6406e-01,\n",
       "                       -1.9495e-01,  1.7273e-01, -2.5464e-01,  3.6768e-01, -2.9102e-01,\n",
       "                       -1.5918e-01,  2.0471e-01, -3.2812e-01,  2.2131e-01, -1.1847e-01,\n",
       "                        2.0642e-01,  3.2324e-01, -4.1895e-01, -1.8042e-01, -2.6733e-01,\n",
       "                       -2.9834e-01, -2.4695e-01, -2.6465e-01,  2.0676e-02,  6.9763e-02,\n",
       "                        1.1255e-01,  3.0957e-01,  5.0293e-01,  3.1421e-01,  1.4185e-01,\n",
       "                       -2.6562e-01, -2.4817e-01, -5.6854e-02,  1.8518e-01, -2.5269e-01,\n",
       "                       -2.2961e-01, -8.8867e-02, -4.0796e-01, -1.9324e-01,  4.3262e-01,\n",
       "                       -3.6523e-01,  4.0016e-03,  1.8518e-01,  2.6416e-01,  2.4414e-01,\n",
       "                        9.4177e-02, -1.8005e-01, -1.4246e-01, -2.9395e-01,  3.0396e-01,\n",
       "                       -3.4131e-01, -4.0039e-01, -2.5732e-01, -3.2959e-01,  1.6553e-01,\n",
       "                        2.8491e-01, -2.8394e-01,  5.0342e-01,  2.0190e-01, -4.0576e-01,\n",
       "                        1.8066e-01,  2.8882e-01,  4.3945e-01,  7.6561e-03, -3.0322e-01,\n",
       "                        2.8101e-01,  1.8518e-01, -3.6682e-02,  1.5088e-01,  2.3340e-01,\n",
       "                       -4.3042e-01, -3.6475e-01, -3.6640e-03, -3.4912e-01, -3.8013e-01,\n",
       "                       -4.0436e-02,  3.5474e-01, -4.4434e-01,  5.0830e-01,  3.2373e-01,\n",
       "                        2.6807e-01,  2.6416e-01, -4.2505e-01,  2.9956e-01,  2.4902e-01,\n",
       "                        2.5269e-01,  3.7646e-01, -4.3091e-01,  3.6304e-01,  1.9238e-01,\n",
       "                        1.1406e-02, -9.9258e-03, -2.1826e-01, -4.3396e-02, -3.0566e-01,\n",
       "                       -3.7842e-01, -1.7554e-01, -5.0293e-01, -3.7109e-01,  3.6548e-01,\n",
       "                       -4.4824e-01,  3.2654e-02, -1.1391e-02,  6.0730e-02,  1.7426e-02,\n",
       "                       -1.6431e-01, -8.7219e-02,  1.8884e-01,  7.8552e-02, -4.7821e-02,\n",
       "                        9.8724e-03, -1.9165e-02, -9.9945e-03, -1.0797e-01, -4.2084e-02,\n",
       "                        3.9635e-03, -2.5732e-01,  1.3252e-02,  7.1289e-02,  1.6113e-02,\n",
       "                       -1.3382e-02, -2.1393e-02,  8.0872e-02, -2.5952e-01,  2.6932e-02,\n",
       "                       -1.4763e-02, -2.5284e-02, -2.7905e-01, -3.7659e-02, -1.9409e-02,\n",
       "                       -8.9050e-02, -9.7733e-03, -6.6223e-02,  3.5400e-02, -7.4654e-03,\n",
       "                        6.4697e-02,  1.6052e-01, -1.9073e-02, -1.9806e-02,  3.6835e-02,\n",
       "                        6.1737e-02, -1.0675e-01,  1.5793e-02,  1.3147e-01,  2.5772e-02,\n",
       "                        3.2501e-02,  3.2440e-02, -4.0619e-02,  4.0359e-03, -5.0140e-02,\n",
       "                       -7.7087e-02,  1.2772e-02,  7.0862e-02, -5.2246e-02,  5.9509e-04,\n",
       "                       -2.0569e-02, -4.2999e-02,  1.4465e-02, -2.6562e-01,  5.4359e-03,\n",
       "                       -9.1858e-02, -6.3171e-02,  1.6586e-02,  4.0649e-02,  8.2214e-02,\n",
       "                        2.0844e-02,  5.9967e-03,  4.3488e-02,  2.6047e-02, -2.3384e-03,\n",
       "                       -1.2207e-02, -2.7603e-02,  5.6250e-01, -1.6510e-02, -1.4435e-02,\n",
       "                        5.2930e-01,  2.1805e-02, -1.0262e-03, -2.6810e-02, -2.4414e-03,\n",
       "                       -2.5978e-03,  5.5615e-01, -1.9653e-02, -3.8357e-03, -1.4435e-02,\n",
       "                       -1.5656e-02, -5.8441e-02, -1.7746e-02, -2.8870e-02,  4.0314e-02,\n",
       "                        7.3671e-04,  2.7588e-02, -2.4231e-02,  1.5434e-02,  7.2289e-03,\n",
       "                       -1.5221e-02,  3.0075e-02,  1.4343e-02,  1.7609e-02, -7.1869e-03,\n",
       "                       -2.0630e-02, -3.5736e-02,  3.8574e-02,  6.7139e-04,  1.4320e-02,\n",
       "                        4.2343e-03, -3.5156e-02,  2.5558e-02,  4.6659e-04, -1.2123e-02,\n",
       "                       -2.3880e-02,  1.8906e-02,  7.2937e-03, -2.8259e-02,  1.5465e-02,\n",
       "                       -5.1074e-01,  3.4149e-02,  1.1234e-03,  1.4786e-02,  1.7639e-02,\n",
       "                        5.1849e-02, -6.3591e-03,  4.0741e-03, -2.8381e-03, -2.1534e-03,\n",
       "                       -3.2532e-02,  4.7424e-02,  1.7365e-02,  5.9853e-03,  1.7651e-01,\n",
       "                       -6.3477e-02, -3.6133e-02, -1.9006e-01,  2.5586e-01, -3.6938e-01,\n",
       "                       -3.5034e-01, -1.9250e-01, -3.6938e-01, -1.6724e-01,  3.4619e-01,\n",
       "                        2.4377e-01,  1.3721e-01,  1.8158e-02,  1.2109e-01,  3.1128e-01,\n",
       "                       -4.5228e-04,  1.8616e-01,  4.4580e-01,  2.2864e-01, -3.3661e-02,\n",
       "                       -2.1655e-01,  4.4507e-01, -1.2964e-01, -1.9812e-01, -2.3779e-01,\n",
       "                       -2.4524e-01,  2.4585e-01, -2.5439e-01,  4.7095e-01, -6.5308e-02,\n",
       "                       -3.3765e-01, -4.6045e-01, -1.1108e-01,  2.8442e-01,  1.3000e-01,\n",
       "                        5.0342e-01,  1.7236e-01,  1.1676e-01,  1.6870e-01,  3.2983e-01,\n",
       "                       -2.1204e-01,  1.1945e-01,  9.6313e-02,  1.2390e-01, -3.0811e-01,\n",
       "                        3.0981e-01, -7.5623e-02,  1.7664e-01,  3.6572e-01,  4.6582e-01,\n",
       "                        1.7114e-01, -2.4744e-01,  7.0496e-02, -3.2837e-02,  3.2568e-01,\n",
       "                        1.4648e-01, -2.9785e-01, -1.1273e-01,  1.7529e-01, -3.9429e-01,\n",
       "                       -1.6736e-01, -3.0609e-02, -4.4824e-01, -6.6284e-02,  1.2543e-02,\n",
       "                       -1.6614e-01,  2.9614e-01, -3.0859e-01, -1.7639e-01,  8.0322e-02,\n",
       "                       -3.1348e-01, -2.6514e-01, -2.7295e-01, -2.3755e-01,  1.0217e-01,\n",
       "                        8.8379e-02,  8.1177e-02,  3.8391e-02, -5.9509e-02,  3.3325e-01,\n",
       "                       -7.1594e-02, -7.3669e-02,  2.3544e-02,  4.9347e-02,  1.7932e-01,\n",
       "                        1.3123e-01,  1.7517e-01, -3.6499e-01,  2.6733e-01, -2.6245e-01,\n",
       "                       -8.0490e-03,  7.1487e-03, -2.1204e-01,  5.1123e-01, -1.5356e-01,\n",
       "                       -4.1382e-01, -1.7810e-01, -6.2866e-02,  1.1420e-01,  8.2520e-02,\n",
       "                       -3.1641e-01, -1.5335e-02, -5.0995e-02, -2.9199e-01, -1.7847e-01,\n",
       "                        4.2145e-02,  2.9663e-01, -7.2144e-02, -1.3220e-01,  3.0322e-01,\n",
       "                       -1.2683e-01, -2.8961e-02,  2.2430e-02,  7.2876e-02,  7.6599e-02,\n",
       "                       -2.7145e-02,  2.5854e-01, -1.2128e-01, -3.8867e-01, -6.0699e-02,\n",
       "                        2.7786e-02, -3.5059e-01,  6.4636e-02,  1.3660e-01,  2.9395e-01,\n",
       "                       -1.2329e-01, -8.0444e-02, -9.2163e-02, -5.5518e-01,  1.5649e-01,\n",
       "                       -5.0732e-01, -5.2539e-01,  5.5127e-01, -2.2064e-02, -6.2158e-01,\n",
       "                        5.0586e-01, -5.2490e-01,  1.0199e-01, -2.4683e-01,  5.1807e-01,\n",
       "                        3.6499e-01, -5.5615e-01,  5.6299e-01, -1.1469e-01, -5.1172e-01,\n",
       "                        1.5137e-01,  5.5811e-01, -5.7861e-01, -5.0928e-01,  3.2617e-01,\n",
       "                        2.3938e-01, -5.7666e-01,  1.6016e-01, -2.5488e-01, -3.4863e-01,\n",
       "                        2.4780e-01,  3.9337e-02, -1.6858e-01, -8.2825e-02, -3.3325e-02,\n",
       "                        5.0146e-01,  2.9688e-01,  1.3110e-01,  5.7812e-01,  2.0154e-01,\n",
       "                        8.7585e-02, -5.1416e-01, -2.4902e-01, -1.6833e-01,  5.3760e-01,\n",
       "                       -5.3857e-01,  5.6915e-02,  5.0195e-01,  1.3464e-01,  5.0586e-01,\n",
       "                        6.7383e-02,  1.7346e-01, -2.5781e-01, -5.3857e-01,  7.5455e-03,\n",
       "                       -5.0781e-01, -3.9404e-01,  3.0176e-01,  5.0977e-01, -1.2421e-01,\n",
       "                       -5.0293e-01, -1.2347e-01,  5.1416e-01, -5.3320e-01, -5.7080e-01,\n",
       "                       -3.4644e-01,  5.0781e-01, -7.8796e-02, -4.2236e-02, -2.6221e-01,\n",
       "                        1.0114e-01,  1.0971e-02, -7.5562e-02, -2.2595e-01, -1.4502e-01,\n",
       "                        3.7671e-01,  3.6938e-01,  1.5576e-01,  5.8632e-03,  1.6602e-01,\n",
       "                        2.8955e-01, -4.7217e-01, -1.1194e-01,  2.9077e-01,  2.6758e-01,\n",
       "                       -6.3293e-02,  1.1340e-01,  5.3174e-01,  4.0210e-01,  1.3965e-01,\n",
       "                        1.1505e-01,  8.7891e-02, -8.8562e-02, -4.9023e-01,  3.9380e-01,\n",
       "                        1.3611e-01,  2.3767e-01,  1.9424e-02, -5.5322e-01,  2.3218e-01,\n",
       "                       -5.2881e-01, -1.0211e-01,  2.5854e-01, -3.2520e-01,  1.6632e-02,\n",
       "                       -2.7979e-01,  2.8857e-01, -7.1289e-02, -5.2979e-01,  2.5195e-01,\n",
       "                       -3.1226e-01, -2.7466e-03,  1.2793e-01,  9.7168e-02,  6.2103e-02,\n",
       "                        9.3567e-02,  2.4023e-01,  4.6069e-01, -5.0926e-03,  2.7271e-01,\n",
       "                        1.3184e-01, -2.3901e-01, -3.5205e-01, -1.8982e-01,  3.2806e-02,\n",
       "                        1.8774e-01, -1.1591e-01, -5.9021e-02,  2.8702e-02, -3.8379e-01,\n",
       "                        1.9470e-01,  2.2986e-01,  1.3220e-01,  1.9446e-01,  3.7292e-02,\n",
       "                        1.5076e-01, -6.5308e-02, -3.5461e-02, -7.5867e-02,  1.3379e-01,\n",
       "                        1.0284e-01, -2.7740e-02, -4.9324e-03,  1.3000e-01, -9.5032e-02,\n",
       "                        2.3041e-02,  4.3884e-02,  1.4294e-01, -5.5054e-02,  6.8909e-02,\n",
       "                        5.5939e-02,  6.2408e-02, -2.1582e-01,  2.5000e-01,  5.3076e-01,\n",
       "                       -2.2791e-01, -2.5049e-01,  1.6138e-01, -9.7229e-02, -2.1667e-01,\n",
       "                       -2.4976e-01, -2.5146e-01, -5.2643e-02, -1.2457e-01, -7.0229e-03,\n",
       "                       -1.5002e-01, -8.4473e-02, -9.4681e-03,  8.1055e-02,  9.6252e-02,\n",
       "                        1.7773e-01, -8.1604e-02,  1.3806e-01, -1.8677e-01,  1.6895e-01,\n",
       "                       -3.7628e-02,  5.9662e-02, -9.2773e-02, -7.1533e-02, -1.9885e-01,\n",
       "                        5.9174e-02,  2.5000e-01, -2.3514e-02,  7.3608e-02,  2.2290e-01,\n",
       "                        6.6772e-02, -1.2500e-01, -1.3550e-01, -6.0028e-02, -1.3147e-01,\n",
       "                       -1.2756e-01,  6.3416e-02,  1.3062e-01,  1.2488e-01,  6.1584e-02,\n",
       "                       -6.7101e-03, -3.3875e-02, -1.6556e-02,  1.8738e-02, -4.7836e-03,\n",
       "                       -3.2776e-02,  1.4427e-02, -5.1221e-01, -4.6661e-02,  3.5343e-03,\n",
       "                        1.5762e-02,  2.3918e-03, -3.1250e-02, -1.8661e-02, -3.0685e-02,\n",
       "                        1.5762e-02, -8.2779e-03, -7.0190e-02,  1.4915e-02, -1.6663e-02,\n",
       "                        2.6138e-02, -8.9645e-03,  1.2150e-03, -1.5222e-01, -2.7023e-02,\n",
       "                       -2.4277e-02, -8.3237e-03,  2.5192e-02, -7.4120e-03, -7.1487e-03,\n",
       "                       -3.6377e-01,  7.0496e-02, -2.0630e-01,  2.1744e-02,  2.2049e-02,\n",
       "                       -1.7487e-02, -5.1910e-02,  3.1799e-02,  1.8585e-02,  2.3880e-02,\n",
       "                       -1.8433e-02,  9.0866e-03,  1.8753e-02,  1.7273e-02, -4.1580e-03,\n",
       "                        1.5976e-02, -2.8519e-02,  4.3030e-03, -2.9312e-02,  3.5217e-02,\n",
       "                        4.0771e-02, -1.8188e-01,  9.0103e-03,  5.0586e-01,  2.0020e-02,\n",
       "                        1.9806e-02,  1.6861e-02,  2.5436e-02,  1.7548e-02, -8.7357e-03,\n",
       "                       -1.6785e-02,  1.8906e-02, -1.3695e-02], device='cuda:0',\n",
       "                      dtype=torch.float16))]),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'infeatures': 768,\n",
       " 'outfeatures': 768,\n",
       " 'bits': 4,\n",
       " 'group_size': 128,\n",
       " 'maxq': 15,\n",
       " 'half_indim': 384,\n",
       " 'use_cuda_fp16': True,\n",
       " 'wf': tensor([[ 0,  4,  8, 12, 16, 20, 24, 28]], dtype=torch.int32),\n",
       " 'kernel_switch_threshold': 128,\n",
       " 'autogptq_cuda_available': True,\n",
       " 'autogptq_cuda': <module 'autogptq_cuda_256' from '/opt/conda/envs/phb/lib/python3.10/site-packages/autogptq_cuda_256.cpython-310-x86_64-linux-gnu.so'>,\n",
       " 'trainable': False,\n",
       " 'device': device(type='cpu')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../static/微信图片_20240305102356.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调用量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig,AwqConfig\n",
    "import torch\n",
    "model_path='/mnt/data/opt-125m'\n",
    "quant_path='/mnt/phb/quant'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Some weights of the model checkpoint at /mnt/phb/quant were not used when initializing OPTForCausalLM: ['model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing OPTForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OPTForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of OPTForCausalLM were not initialized from the model checkpoint at /mnt/phb/quant and are newly initialized: ['model.decoder.layers.0.fc1.qweight', 'model.decoder.layers.0.fc1.qzeros', 'model.decoder.layers.0.fc1.scales', 'model.decoder.layers.0.fc2.qweight', 'model.decoder.layers.0.fc2.qzeros', 'model.decoder.layers.0.fc2.scales', 'model.decoder.layers.0.self_attn.k_proj.qweight', 'model.decoder.layers.0.self_attn.k_proj.qzeros', 'model.decoder.layers.0.self_attn.k_proj.scales', 'model.decoder.layers.0.self_attn.out_proj.qweight', 'model.decoder.layers.0.self_attn.out_proj.qzeros', 'model.decoder.layers.0.self_attn.out_proj.scales', 'model.decoder.layers.0.self_attn.q_proj.qweight', 'model.decoder.layers.0.self_attn.q_proj.qzeros', 'model.decoder.layers.0.self_attn.q_proj.scales', 'model.decoder.layers.0.self_attn.v_proj.qweight', 'model.decoder.layers.0.self_attn.v_proj.qzeros', 'model.decoder.layers.0.self_attn.v_proj.scales', 'model.decoder.layers.1.fc1.qweight', 'model.decoder.layers.1.fc1.qzeros', 'model.decoder.layers.1.fc1.scales', 'model.decoder.layers.1.fc2.qweight', 'model.decoder.layers.1.fc2.qzeros', 'model.decoder.layers.1.fc2.scales', 'model.decoder.layers.1.self_attn.k_proj.qweight', 'model.decoder.layers.1.self_attn.k_proj.qzeros', 'model.decoder.layers.1.self_attn.k_proj.scales', 'model.decoder.layers.1.self_attn.out_proj.qweight', 'model.decoder.layers.1.self_attn.out_proj.qzeros', 'model.decoder.layers.1.self_attn.out_proj.scales', 'model.decoder.layers.1.self_attn.q_proj.qweight', 'model.decoder.layers.1.self_attn.q_proj.qzeros', 'model.decoder.layers.1.self_attn.q_proj.scales', 'model.decoder.layers.1.self_attn.v_proj.qweight', 'model.decoder.layers.1.self_attn.v_proj.qzeros', 'model.decoder.layers.1.self_attn.v_proj.scales', 'model.decoder.layers.10.fc1.qweight', 'model.decoder.layers.10.fc1.qzeros', 'model.decoder.layers.10.fc1.scales', 'model.decoder.layers.10.fc2.qweight', 'model.decoder.layers.10.fc2.qzeros', 'model.decoder.layers.10.fc2.scales', 'model.decoder.layers.10.self_attn.k_proj.qweight', 'model.decoder.layers.10.self_attn.k_proj.qzeros', 'model.decoder.layers.10.self_attn.k_proj.scales', 'model.decoder.layers.10.self_attn.out_proj.qweight', 'model.decoder.layers.10.self_attn.out_proj.qzeros', 'model.decoder.layers.10.self_attn.out_proj.scales', 'model.decoder.layers.10.self_attn.q_proj.qweight', 'model.decoder.layers.10.self_attn.q_proj.qzeros', 'model.decoder.layers.10.self_attn.q_proj.scales', 'model.decoder.layers.10.self_attn.v_proj.qweight', 'model.decoder.layers.10.self_attn.v_proj.qzeros', 'model.decoder.layers.10.self_attn.v_proj.scales', 'model.decoder.layers.11.fc1.qweight', 'model.decoder.layers.11.fc1.qzeros', 'model.decoder.layers.11.fc1.scales', 'model.decoder.layers.11.fc2.qweight', 'model.decoder.layers.11.fc2.qzeros', 'model.decoder.layers.11.fc2.scales', 'model.decoder.layers.11.self_attn.k_proj.qweight', 'model.decoder.layers.11.self_attn.k_proj.qzeros', 'model.decoder.layers.11.self_attn.k_proj.scales', 'model.decoder.layers.11.self_attn.out_proj.qweight', 'model.decoder.layers.11.self_attn.out_proj.qzeros', 'model.decoder.layers.11.self_attn.out_proj.scales', 'model.decoder.layers.11.self_attn.q_proj.qweight', 'model.decoder.layers.11.self_attn.q_proj.qzeros', 'model.decoder.layers.11.self_attn.q_proj.scales', 'model.decoder.layers.11.self_attn.v_proj.qweight', 'model.decoder.layers.11.self_attn.v_proj.qzeros', 'model.decoder.layers.11.self_attn.v_proj.scales', 'model.decoder.layers.2.fc1.qweight', 'model.decoder.layers.2.fc1.qzeros', 'model.decoder.layers.2.fc1.scales', 'model.decoder.layers.2.fc2.qweight', 'model.decoder.layers.2.fc2.qzeros', 'model.decoder.layers.2.fc2.scales', 'model.decoder.layers.2.self_attn.k_proj.qweight', 'model.decoder.layers.2.self_attn.k_proj.qzeros', 'model.decoder.layers.2.self_attn.k_proj.scales', 'model.decoder.layers.2.self_attn.out_proj.qweight', 'model.decoder.layers.2.self_attn.out_proj.qzeros', 'model.decoder.layers.2.self_attn.out_proj.scales', 'model.decoder.layers.2.self_attn.q_proj.qweight', 'model.decoder.layers.2.self_attn.q_proj.qzeros', 'model.decoder.layers.2.self_attn.q_proj.scales', 'model.decoder.layers.2.self_attn.v_proj.qweight', 'model.decoder.layers.2.self_attn.v_proj.qzeros', 'model.decoder.layers.2.self_attn.v_proj.scales', 'model.decoder.layers.3.fc1.qweight', 'model.decoder.layers.3.fc1.qzeros', 'model.decoder.layers.3.fc1.scales', 'model.decoder.layers.3.fc2.qweight', 'model.decoder.layers.3.fc2.qzeros', 'model.decoder.layers.3.fc2.scales', 'model.decoder.layers.3.self_attn.k_proj.qweight', 'model.decoder.layers.3.self_attn.k_proj.qzeros', 'model.decoder.layers.3.self_attn.k_proj.scales', 'model.decoder.layers.3.self_attn.out_proj.qweight', 'model.decoder.layers.3.self_attn.out_proj.qzeros', 'model.decoder.layers.3.self_attn.out_proj.scales', 'model.decoder.layers.3.self_attn.q_proj.qweight', 'model.decoder.layers.3.self_attn.q_proj.qzeros', 'model.decoder.layers.3.self_attn.q_proj.scales', 'model.decoder.layers.3.self_attn.v_proj.qweight', 'model.decoder.layers.3.self_attn.v_proj.qzeros', 'model.decoder.layers.3.self_attn.v_proj.scales', 'model.decoder.layers.4.fc1.qweight', 'model.decoder.layers.4.fc1.qzeros', 'model.decoder.layers.4.fc1.scales', 'model.decoder.layers.4.fc2.qweight', 'model.decoder.layers.4.fc2.qzeros', 'model.decoder.layers.4.fc2.scales', 'model.decoder.layers.4.self_attn.k_proj.qweight', 'model.decoder.layers.4.self_attn.k_proj.qzeros', 'model.decoder.layers.4.self_attn.k_proj.scales', 'model.decoder.layers.4.self_attn.out_proj.qweight', 'model.decoder.layers.4.self_attn.out_proj.qzeros', 'model.decoder.layers.4.self_attn.out_proj.scales', 'model.decoder.layers.4.self_attn.q_proj.qweight', 'model.decoder.layers.4.self_attn.q_proj.qzeros', 'model.decoder.layers.4.self_attn.q_proj.scales', 'model.decoder.layers.4.self_attn.v_proj.qweight', 'model.decoder.layers.4.self_attn.v_proj.qzeros', 'model.decoder.layers.4.self_attn.v_proj.scales', 'model.decoder.layers.5.fc1.qweight', 'model.decoder.layers.5.fc1.qzeros', 'model.decoder.layers.5.fc1.scales', 'model.decoder.layers.5.fc2.qweight', 'model.decoder.layers.5.fc2.qzeros', 'model.decoder.layers.5.fc2.scales', 'model.decoder.layers.5.self_attn.k_proj.qweight', 'model.decoder.layers.5.self_attn.k_proj.qzeros', 'model.decoder.layers.5.self_attn.k_proj.scales', 'model.decoder.layers.5.self_attn.out_proj.qweight', 'model.decoder.layers.5.self_attn.out_proj.qzeros', 'model.decoder.layers.5.self_attn.out_proj.scales', 'model.decoder.layers.5.self_attn.q_proj.qweight', 'model.decoder.layers.5.self_attn.q_proj.qzeros', 'model.decoder.layers.5.self_attn.q_proj.scales', 'model.decoder.layers.5.self_attn.v_proj.qweight', 'model.decoder.layers.5.self_attn.v_proj.qzeros', 'model.decoder.layers.5.self_attn.v_proj.scales', 'model.decoder.layers.6.fc1.qweight', 'model.decoder.layers.6.fc1.qzeros', 'model.decoder.layers.6.fc1.scales', 'model.decoder.layers.6.fc2.qweight', 'model.decoder.layers.6.fc2.qzeros', 'model.decoder.layers.6.fc2.scales', 'model.decoder.layers.6.self_attn.k_proj.qweight', 'model.decoder.layers.6.self_attn.k_proj.qzeros', 'model.decoder.layers.6.self_attn.k_proj.scales', 'model.decoder.layers.6.self_attn.out_proj.qweight', 'model.decoder.layers.6.self_attn.out_proj.qzeros', 'model.decoder.layers.6.self_attn.out_proj.scales', 'model.decoder.layers.6.self_attn.q_proj.qweight', 'model.decoder.layers.6.self_attn.q_proj.qzeros', 'model.decoder.layers.6.self_attn.q_proj.scales', 'model.decoder.layers.6.self_attn.v_proj.qweight', 'model.decoder.layers.6.self_attn.v_proj.qzeros', 'model.decoder.layers.6.self_attn.v_proj.scales', 'model.decoder.layers.7.fc1.qweight', 'model.decoder.layers.7.fc1.qzeros', 'model.decoder.layers.7.fc1.scales', 'model.decoder.layers.7.fc2.qweight', 'model.decoder.layers.7.fc2.qzeros', 'model.decoder.layers.7.fc2.scales', 'model.decoder.layers.7.self_attn.k_proj.qweight', 'model.decoder.layers.7.self_attn.k_proj.qzeros', 'model.decoder.layers.7.self_attn.k_proj.scales', 'model.decoder.layers.7.self_attn.out_proj.qweight', 'model.decoder.layers.7.self_attn.out_proj.qzeros', 'model.decoder.layers.7.self_attn.out_proj.scales', 'model.decoder.layers.7.self_attn.q_proj.qweight', 'model.decoder.layers.7.self_attn.q_proj.qzeros', 'model.decoder.layers.7.self_attn.q_proj.scales', 'model.decoder.layers.7.self_attn.v_proj.qweight', 'model.decoder.layers.7.self_attn.v_proj.qzeros', 'model.decoder.layers.7.self_attn.v_proj.scales', 'model.decoder.layers.8.fc1.qweight', 'model.decoder.layers.8.fc1.qzeros', 'model.decoder.layers.8.fc1.scales', 'model.decoder.layers.8.fc2.qweight', 'model.decoder.layers.8.fc2.qzeros', 'model.decoder.layers.8.fc2.scales', 'model.decoder.layers.8.self_attn.k_proj.qweight', 'model.decoder.layers.8.self_attn.k_proj.qzeros', 'model.decoder.layers.8.self_attn.k_proj.scales', 'model.decoder.layers.8.self_attn.out_proj.qweight', 'model.decoder.layers.8.self_attn.out_proj.qzeros', 'model.decoder.layers.8.self_attn.out_proj.scales', 'model.decoder.layers.8.self_attn.q_proj.qweight', 'model.decoder.layers.8.self_attn.q_proj.qzeros', 'model.decoder.layers.8.self_attn.q_proj.scales', 'model.decoder.layers.8.self_attn.v_proj.qweight', 'model.decoder.layers.8.self_attn.v_proj.qzeros', 'model.decoder.layers.8.self_attn.v_proj.scales', 'model.decoder.layers.9.fc1.qweight', 'model.decoder.layers.9.fc1.qzeros', 'model.decoder.layers.9.fc1.scales', 'model.decoder.layers.9.fc2.qweight', 'model.decoder.layers.9.fc2.qzeros', 'model.decoder.layers.9.fc2.scales', 'model.decoder.layers.9.self_attn.k_proj.qweight', 'model.decoder.layers.9.self_attn.k_proj.qzeros', 'model.decoder.layers.9.self_attn.k_proj.scales', 'model.decoder.layers.9.self_attn.out_proj.qweight', 'model.decoder.layers.9.self_attn.out_proj.qzeros', 'model.decoder.layers.9.self_attn.out_proj.scales', 'model.decoder.layers.9.self_attn.q_proj.qweight', 'model.decoder.layers.9.self_attn.q_proj.qzeros', 'model.decoder.layers.9.self_attn.q_proj.scales', 'model.decoder.layers.9.self_attn.v_proj.qweight', 'model.decoder.layers.9.self_attn.v_proj.qzeros', 'model.decoder.layers.9.self_attn.v_proj.scales']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(quant_path,trust_remote_code=True).to(device)\n",
    "tokenizer=AutoTokenizer.from_pretrained(quant_path,trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    print(inputs)\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    print(out)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 42891]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/phb/fine-tunning/model/model_quant.ipynb Cell 42\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/phb/fine-tunning/model/model_quant.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m generate_text(\u001b[39m'\u001b[39;49m\u001b[39mhello\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/mnt/phb/fine-tunning/model/model_quant.ipynb Cell 42\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/phb/fine-tunning/model/model_quant.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/phb/fine-tunning/model/model_quant.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs)\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/phb/fine-tunning/model/model_quant.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/phb/fine-tunning/model/model_quant.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(out)\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/phb/fine-tunning/model/model_quant.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(out[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1545\u001b[0m         input_ids,\n\u001b[1;32m   1546\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1547\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1548\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1549\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1550\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1551\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1552\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1553\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1554\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1555\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1556\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2405\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2406\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2407\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2408\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2409\u001b[0m )\n\u001b[1;32m   2411\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:1145\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1142\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1144\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1145\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1146\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1147\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1148\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1149\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1150\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1151\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1152\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1153\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1154\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1155\u001b[0m )\n\u001b[1;32m   1157\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m   1159\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:911\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    901\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    902\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    903\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m         use_cache,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    910\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[1;32m    913\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    914\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    915\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    916\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    917\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    920\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:552\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    549\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    551\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    553\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    554\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    555\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    556\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    557\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    559\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    560\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:182\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    179\u001b[0m bsz, tgt_len, _ \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39msize()\n\u001b[1;32m    181\u001b[0m \u001b[39m# get query proj\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m query_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_proj(hidden_states) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling\n\u001b[1;32m    183\u001b[0m \u001b[39m# get key, value proj\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m is_cross_attention \u001b[39mand\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39m# reuse k,v, cross_attentions\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/awq/modules/linear/gemm.py:242\u001b[0m, in \u001b[0;36mWQLinear_GEMM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 242\u001b[0m         out \u001b[39m=\u001b[39m WQLinearMMFunction\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    243\u001b[0m             x,\n\u001b[1;32m    244\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqweight,\n\u001b[1;32m    245\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqzeros,\n\u001b[1;32m    246\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscales,\n\u001b[1;32m    247\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_bit,\n\u001b[1;32m    248\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup_size,\n\u001b[1;32m    249\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    250\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_features,\n\u001b[1;32m    251\u001b[0m         )\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m input_dtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n\u001b[1;32m    254\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39minput_dtype)\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/phb/lib/python3.10/site-packages/awq/modules/linear/gemm.py:46\u001b[0m, in \u001b[0;36mWQLinearMMFunction.forward\u001b[0;34m(ctx, x, qweight, qzeros, scales, w_bit, group_size, bias, out_features)\u001b[0m\n\u001b[1;32m     44\u001b[0m         out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(x, out)\n\u001b[1;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         out \u001b[39m=\u001b[39m awq_ext\u001b[39m.\u001b[39;49mgemm_forward_cuda(\n\u001b[1;32m     47\u001b[0m             x\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), qweight, scales, qzeros, \u001b[39m8\u001b[39;49m\n\u001b[1;32m     48\u001b[0m         )\n\u001b[1;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     out \u001b[39m=\u001b[39m dequantize_gemm(qweight, qzeros, scales, w_bit, group_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "generate_text('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
