{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model和tokenizer的加载\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非常建议去看官方文档  [HuggingFace的AutoClass文档](https://huggingface.co/docs/transformers/model_doc/auto#auto-classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词器tokenizer一般使用Autotokenizer加载,使用from_pretrained加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![这是图片](../static/屏幕截图%202024-03-03%20161245.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型model一般使用Automodel或者使用AutoModelForCausalLM加载,\n",
    "### 使用from_pretrained或者from_config加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![图片](../static/屏幕截图%202024-03-03%20161437.png)\n",
    "![图片](../static/屏幕截图%202024-03-03%20161411.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 把模型加载到gpu可以使用model.cuda()  或者   model.to(device)\n",
    "``` shell\n",
    "model=AutoModelForCausalLM.from_pretrained('E:\\model\\language\\opt-125m',trust_remote_code=True,device_map='auto').cuda()\n",
    "```\n",
    "\n",
    "```shell\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel,AutoModelForCausalLM\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained('E:\\model\\language\\opt-125m',trust_remote_code=True,device_map='auto').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.decoder.embed_tokens.weight data type: torch.float32\n",
      "Parameter model.decoder.embed_positions.weight data type: torch.float32\n",
      "Parameter model.decoder.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.0.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.0.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.1.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.1.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.2.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.2.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.3.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.3.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.4.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.4.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.5.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.5.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.6.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.6.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.7.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.7.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.8.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.8.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.9.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.9.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.10.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.10.final_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.k_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.k_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.v_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.v_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.q_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.q_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.out_proj.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn.out_proj.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.self_attn_layer_norm.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.fc1.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.fc1.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.fc2.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.fc2.bias data type: torch.float32\n",
      "Parameter model.decoder.layers.11.final_layer_norm.weight data type: torch.float32\n",
      "Parameter model.decoder.layers.11.final_layer_norm.bias data type: torch.float32\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for name, param in model.named_parameters():\n",
    "    n=n+1\n",
    "    print(f\"Parameter {name} data type: {param.dtype}\")\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477.75MiB\n"
     ]
    }
   ],
   "source": [
    "# 获取当前模型占用的 GPU显存（差值为预留给 PyTorch 的显存）\n",
    "memory_footprint_bytes = model.get_memory_footprint()\n",
    "memory_footprint_mib = memory_footprint_bytes / (1024 ** 2)  # 转换为 MiB\n",
    "\n",
    "print(f\"{memory_footprint_mib:.2f}MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('E:\\model\\language\\opt-125m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把变量inputs加载入GPU,也是两种方式\n",
    "```shell\n",
    "inputs=inputs.to(device)\n",
    "```\n",
    "```shell\n",
    "inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    inputs=tokenizer(text,return_tensors=\"pt\").to(device)\n",
    "    print(\"inputs:{}\".format(inputs))\n",
    "    output=model.generate(**inputs,max_length=20)\n",
    "    print(\"output:{}\".format(output))\n",
    "    res=tokenizer.decode(output[0],skip_special_tokens=True)\n",
    "    print(\"回答：{}\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:{'input_ids': tensor([[    2, 42891]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n",
      "output:tensor([[    2, 42891,     6,    38,   437,    10,    92,   869,     8,    38,\n",
      "           437,   546,    13,    10,   205,   165,     7,   310,    19,     4]],\n",
      "       device='cuda:0')\n",
      "回答：hello, I'm a new player and I'm looking for a good team to play with.\n"
     ]
    }
   ],
   "source": [
    "chat('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:{'input_ids': tensor([[    2, 47643, 47516, 10809]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}\n",
      "output:tensor([[    2, 47643, 47516, 10809, 47973, 48570,  3602, 48549, 47341, 36714,\n",
      "         15389, 15264, 47516, 10809, 47973, 48570,  3602, 48538, 36714, 15389]],\n",
      "       device='cuda:0')\n",
      "回答：中国人民は、米国人民が�\n"
     ]
    }
   ],
   "source": [
    "chat('中国')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    inputs=tokenizer(text,return_tensors=\"pt\").to(device)\n",
    "    print(\"inputs:{}\".format(inputs))\n",
    "    output=model.generate(**inputs,max_length=20)\n",
    "    print(\"output:{}\".format(output))\n",
    "    res=tokenizer.decode(output[0],skip_special_tokens=True)\n",
    "    print(\"回答：{}\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:{'input_ids': tensor([[    2, 42891]], device='cuda:0'), 'attention_mask': tensor([[1, 1]], device='cuda:0')}\n",
      "output:tensor([[    2, 42891,     6,    38,   437,    10,    92,   869,     8,    38,\n",
      "           437,   546,    13,    10,   205,   165,     7,   310,    19,     4]],\n",
      "       device='cuda:0')\n",
      "回答：hello, I'm a new player and I'm looking for a good team to play with.\n"
     ]
    }
   ],
   "source": [
    "chat('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:{'input_ids': tensor([[    2, 47643, 47516, 10809]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}\n",
      "output:tensor([[    2, 47643, 47516, 10809, 47973, 48570,  3602, 48549, 47341, 36714,\n",
      "         15389, 15264, 47516, 10809, 47973, 48570,  3602, 48538, 36714, 15389]],\n",
      "       device='cuda:0')\n",
      "回答：中国人民は、米国人民が�\n"
     ]
    }
   ],
   "source": [
    "chat('中国')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
